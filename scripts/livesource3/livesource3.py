import urllib.request
from urllib.parse import urlparse
import re  # æ­£åˆ™è¡¨è¾¾å¼å¤„ç†
import os
from datetime import datetime, timedelta, timezone
import random
import opencc  # ç®€ç¹è½¬æ¢
import socket
import time

# ======================
# åˆå§‹åŒ–é…ç½®
# ======================

# åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs('output/livesource3/', exist_ok=True)

# ç®€ç¹è½¬æ¢å™¨åˆå§‹åŒ–
def traditional_to_simplified(text: str) -> str:
    """å°†ç¹ä½“ä¸­æ–‡è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡"""
    converter = opencc.OpenCC('t2s')
    simplified_text = converter.convert(text)
    return simplified_text

# æ‰§è¡Œå¼€å§‹æ—¶é—´è®°å½•
timestart = datetime.now()

# ======================
# æ–‡ä»¶è¯»å–å·¥å…·å‡½æ•°
# ======================

def read_txt_to_array(file_name):
    """è¯»å–æ–‡æœ¬æ–‡ä»¶åˆ°æ•°ç»„ï¼Œè·³è¿‡ç©ºè¡Œ"""
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            lines = file.readlines()
            lines = [line.strip() for line in lines if line.strip()]
            return lines
    except FileNotFoundError:
        print(f"æ–‡ä»¶æœªæ‰¾åˆ°: '{file_name}'")
        return []
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

def read_blacklist_from_txt(file_path):
    """ä»é»‘åå•æ–‡ä»¶ä¸­è¯»å–URLåˆ—è¡¨"""
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()

    blacklist = [line.split(',')[1].strip() for line in lines if ',' in line]
    return blacklist

# ======================
# é»‘åå•ç®¡ç†
# ======================

# åŠ è½½è‡ªåŠ¨å’Œæ‰‹åŠ¨é»‘åå•
blacklist_auto = read_blacklist_from_txt('scripts/livesource3/blacklist/blacklist_auto.txt')
blacklist_manual = read_blacklist_from_txt('scripts/livesource3/blacklist/blacklist_manual.txt')

# åˆå¹¶é»‘åå•ï¼ˆä½¿ç”¨é›†åˆæé«˜æ£€ç´¢æ•ˆç‡ï¼‰
combined_blacklist = set(blacklist_auto + blacklist_manual)

# =====================
# æ•°æ®å­˜å‚¨å®¹å™¨
# =====================

# æŒ‰é¢‘é“åˆ†ç±»å­˜å‚¨ç›´æ’­æº
ys_lines = [] #CCTV
ws_lines = [] #å«è§†é¢‘é“
sh_lines = [] #åœ°æ–¹å°-ä¸Šæµ·é¢‘é“
zj_lines = [] #åœ°æ–¹å°-æµ™æ±Ÿé¢‘é“
jsu_lines = [] #åœ°æ–¹å°-æ±Ÿè‹é¢‘é“
gd_lines = [] #åœ°æ–¹å°-å¹¿ä¸œé¢‘é“
hn_lines = [] #åœ°æ–¹å°-æ¹–å—é¢‘é“
ah_lines = [] #åœ°æ–¹å°-å®‰å¾½é¢‘é“
hain_lines = [] #åœ°æ–¹å°-æµ·å—é¢‘é“
nm_lines = [] #åœ°æ–¹å°-å†…è’™é¢‘é“
hb_lines = [] #åœ°æ–¹å°-æ¹–åŒ—é¢‘é“
ln_lines = [] #åœ°æ–¹å°-è¾½å®é¢‘é“
sx_lines = [] #åœ°æ–¹å°-é™•è¥¿é¢‘é“
shanxi_lines = [] #åœ°æ–¹å°-å±±è¥¿é¢‘é“
shandong_lines = [] #åœ°æ–¹å°-å±±ä¸œé¢‘é“
yunnan_lines = [] #åœ°æ–¹å°-äº‘å—é¢‘é“
bj_lines = [] #åœ°æ–¹å°-åŒ—äº¬é¢‘é“
cq_lines = [] #åœ°æ–¹å°-é‡åº†é¢‘é“
fj_lines = [] #åœ°æ–¹å°-ç¦å»ºé¢‘é“
gs_lines = [] #åœ°æ–¹å°-ç”˜è‚ƒé¢‘é“
gx_lines = [] #åœ°æ–¹å°-å¹¿è¥¿é¢‘é“
gz_lines = [] #åœ°æ–¹å°-è´µå·é¢‘é“
heb_lines = [] #åœ°æ–¹å°-æ²³åŒ—é¢‘é“
hen_lines = [] #åœ°æ–¹å°-æ²³å—é¢‘é“
hlj_lines = [] #åœ°æ–¹å°-é»‘é¾™æ±Ÿé¢‘é“
jl_lines = [] #åœ°æ–¹å°-å‰æ—é¢‘é“
jx_lines = [] #åœ°æ–¹å°-æ±Ÿè¥¿é¢‘é“
nx_lines = [] #åœ°æ–¹å°-å®å¤é¢‘é“
qh_lines = [] #åœ°æ–¹å°-é’æµ·é¢‘é“
sc_lines = [] #åœ°æ–¹å°-å››å·é¢‘é“
tj_lines = [] #åœ°æ–¹å°-å¤©æ´¥é¢‘é“
xj_lines = [] #åœ°æ–¹å°-æ–°ç–†é¢‘é“

ty_lines = [] #ä½“è‚²é¢‘é“
tyss_lines = [] #ä½“è‚²èµ›äº‹
sz_lines = [] #æ•°å­—é¢‘é“
yy_lines = [] #éŸ³ä¹é¢‘é“
gj_lines = [] #å›½é™…é¢‘é“
js_lines = [] #è§£è¯´
cw_lines = [] #æ˜¥æ™š
dy_lines = [] #ç”µå½±
dsj_lines = [] #ç”µè§†å‰§
gat_lines = [] #æ¸¯æ¾³å°
xg_lines = [] #é¦™æ¸¯
aomen_lines = [] #æ¾³é—¨
tw_lines = [] #å°æ¹¾
dhp_lines = [] #åŠ¨ç”»ç‰‡
douyu_lines = [] #æ–—é±¼ç›´æ’­
huya_lines = [] #è™ç‰™ç›´æ’­
radio_lines = [] #æ”¶éŸ³æœº
zb_lines = [] #ç›´æ’­ä¸­å›½
# favorite_lines = [] #æ”¶è—é¢‘é“
zy_lines = [] #ç»¼è‰ºé¢‘é“
game_lines = [] #æ¸¸æˆé¢‘é“
xq_lines = [] #æˆæ›²
jlp_lines = [] #è®°å½•ç‰‡

other_lines = []
other_lines_url = [] # ä¸ºé™ä½otheræ–‡ä»¶å¤§å°ï¼Œå‰”é™¤é‡å¤urlæ·»åŠ 

# ======================
# é¢‘é“åç§°å¤„ç†å‡½æ•°
# ======================

def process_name_string(input_str):
    """å¤„ç†é¢‘é“åç§°å­—ç¬¦ä¸²"""
    parts = input_str.split(',')
    processed_parts = []
    for part in parts:
        processed_part = process_part(part)
        processed_parts.append(processed_part)
    result_str = ','.join(processed_parts)
    return result_str

def process_part(part_str):
    """å¤„ç†å•ä¸ªé¢‘é“åç§°éƒ¨åˆ†"""
    # CCTVé¢‘é“ç‰¹æ®Šå¤„ç†
    if "CCTV" in part_str and "://" not in part_str:
        part_str = part_str.replace("IPV6", "").replace("PLUS", "+").replace("1080", "")
        filtered_str = ''.join(char for char in part_str if char.isdigit() or char == 'K' or char == '+')
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°å­—ï¼Œè¿”å›åŸåç§°
        if not filtered_str.strip():
            filtered_str = part_str.replace("CCTV", "")
        
        # 4K/8Kç‰¹æ®Šå¤„ç†
        if len(filtered_str) > 2 and re.search(r'4K|8K', filtered_str):
            filtered_str = re.sub(r'(4K|8K).*', r'\1', filtered_str)
            if len(filtered_str) > 2:
                filtered_str = re.sub(r'(4K|8K)', r'(\1)', filtered_str)
        
        return "CCTV" + filtered_str
    
    # å«è§†é¢‘é“å¤„ç†
    elif "å«è§†" in part_str:
        pattern = r'å«è§†ã€Œ.*ã€'
        result_str = re.sub(pattern, 'å«è§†', part_str)
        return result_str
    
    return part_str

# =======================
# URLå¤„ç†å·¥å…·å‡½æ•°
# =======================

def get_url_file_extension(url):
    """è·å–URLçš„æ–‡ä»¶æ‰©å±•å"""
    parsed_url = urlparse(url)
    path = parsed_url.path
    extension = os.path.splitext(path)[1]
    return extension

def convert_m3u_to_txt(m3u_content):
    """å°†M3Uæ ¼å¼è½¬æ¢ä¸ºTXTæ ¼å¼"""
    lines = m3u_content.split('\n')
    txt_lines = []
    channel_name = ""
    
    for line in lines:
        # è·³è¿‡M3Uå¤´
        if line.startswith("#EXTM3U"):
            continue
        
        # å¤„ç†é¢‘é“ä¿¡æ¯è¡Œ
        if line.startswith("#EXTINF"):
            channel_name = line.split(',')[-1].strip()
        
        # å¤„ç†URLè¡Œ
        elif line.startswith("http") or line.startswith("rtmp") or line.startswith("p3p"):
            txt_lines.append(f"{channel_name},{line.strip()}")
        
        # å¤„ç†ç±»TXTæ ¼å¼çš„M3Uæ–‡ä»¶
        if "#genre#" not in line and "," in line and "://" in line:
            pattern = r'^[^,]+,[^\s]+://[^\s]+$'
            if bool(re.match(pattern, line)):
                txt_lines.append(line)
    
    return '\n'.join(txt_lines)

def check_url_existence(data_list, url):
    """æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨äºåˆ—è¡¨ä¸­"""
    urls = [item.split(',')[1] for item in data_list]
    return url not in urls  # ä¸å­˜åœ¨è¿”å›True

def clean_url(url):
    """æ¸…ç†URLï¼Œç§»é™¤$ç¬¦å·åŠå…¶åé¢çš„å†…å®¹"""
    last_dollar_index = url.rfind('$')
    if last_dollar_index != -1:
        return url[:last_dollar_index]
    return url

# =====================
# é¢‘é“åç§°æ¸…ç†
# =====================

# éœ€è¦ä»é¢‘é“åç§°ä¸­ç§»é™¤çš„å­—ç¬¦åˆ—è¡¨
removal_list = [
    "_ç”µä¿¡", "ç”µä¿¡", "é«˜æ¸…", "é¢‘é“", "ï¼ˆHDï¼‰", "-HD", "è‹±é™†", "_ITV", "(åŒ—ç¾)", "(HK)", "AKtv",
    "ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "é¢‘é™†", "å¤‡é™†", "å£¹é™†", "è´°é™†", "åé™†", "è‚†é™†", "ä¼é™†", "é™†é™†", "æŸ’é™†",
    "é¢‘æ™´", "é¢‘ç²¤", "[è¶…æ¸…]", "é«˜æ¸…", "è¶…æ¸…", "æ ‡æ¸…", "æ–¯ç‰¹", "ç²¤é™†", "å›½é™†", "è‚†æŸ’", "é¢‘è‹±",
    "é¢‘ç‰¹", "é¢‘å›½", "é¢‘å£¹", "é¢‘è´°", "è‚†è´°", "é¢‘æµ‹", "å’ªå’•", "é—½ç‰¹", "é«˜ç‰¹", "é¢‘é«˜", "é¢‘æ ‡", "æ±é˜³"
]

def clean_channel_name(channel_name, removal_list):
    """æ¸…ç†é¢‘é“åç§°ä¸­çš„ç‰¹å®šå­—ç¬¦"""
    # 1. ç§»é™¤ç‰¹å®šå­—ç¬¦åˆ—è¡¨
    for item in removal_list:
        channel_name = channel_name.replace(item, "")
    
    # 2. ç§»é™¤æœ«å°¾çš„HD
    if channel_name.endswith("HD"):
        channel_name = channel_name[:-2]
    
    # 3. ç§»é™¤æœ«å°¾çš„"å°"ï¼ˆå¦‚æœåç§°é•¿åº¦å¤§äº3ï¼‰
    if channel_name.endswith("å°") and len(channel_name) > 3:
        channel_name = channel_name[:-1]
    
    # 4. ç§»é™¤å¼€å¤´çš„BDå’ŒHD
    if channel_name.startswith("BD"):
        channel_name = channel_name[2:]  # ç§»é™¤å¼€å¤´"BD"
    elif channel_name.startswith("HD"):
        channel_name = channel_name[2:]  # ç§»é™¤å¼€å¤´"HD"
    
    # 5. æœ€ç»ˆè¿”å›å¤„ç†åçš„åç§°
    return channel_name

# ====================
# ç›´æ’­æºåˆ†ç±»å¤„ç†
# ====================

def process_channel_line(line):
    """å¤„ç†å•è¡Œç›´æ’­æºæ•°æ®å¹¶åˆ†ç±»å­˜å‚¨"""
    if "#genre#" not in line and "#EXTINF:" not in line and "," in line and "://" in line:
        channel_name = line.split(',')[0].strip()
        channel_name = clean_channel_name(channel_name, removal_list)  # æ¸…ç†é¢‘é“åç§°
        channel_name = traditional_to_simplified(channel_name)  # ç¹è½¬ç®€
        
        channel_address = clean_url(line.split(',')[1].strip())  # æ¸…ç†URL
        line = channel_name + "," + channel_address  # é‡æ–°ç»„åˆè¡Œ
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
        if channel_address not in combined_blacklist:
            # æ ¹æ®é¢‘é“åç§°åˆ†ç±»å­˜å‚¨
            if "CCTV" in channel_name and check_url_existence(ys_lines, channel_address) : #å¤®è§†é¢‘é“
                ys_lines.append(process_name_string(line.strip()))
            elif channel_name in ws_dictionary and check_url_existence(ws_lines, channel_address): #å«è§†é¢‘é“
                ws_lines.append(process_name_string(line.strip()))
            elif channel_name in zj_dictionary and check_url_existence(zj_lines, channel_address):  #åœ°æ–¹å°-æµ™æ±Ÿé¢‘é“
                zj_lines.append(process_name_string(line.strip()))
            elif channel_name in jsu_dictionary and check_url_existence(jsu_lines, channel_address):  #åœ°æ–¹å°-æ±Ÿè‹é¢‘é“
                jsu_lines.append(process_name_string(line.strip()))
            elif channel_name in gd_dictionary and check_url_existence(gd_lines, channel_address):  #åœ°æ–¹å°-å¹¿ä¸œé¢‘é“
                gd_lines.append(process_name_string(line.strip()))
            elif channel_name in hn_dictionary and check_url_existence(hn_lines, channel_address):  #åœ°æ–¹å°-æ¹–å—é¢‘é“
                hn_lines.append(process_name_string(line.strip()))
            elif channel_name in hb_dictionary and check_url_existence(hb_lines, channel_address):  #åœ°æ–¹å°-æ¹–åŒ—é¢‘é“
                hb_lines.append(process_name_string(line.strip()))
            elif channel_name in ah_dictionary and check_url_existence(ah_lines, channel_address):  #åœ°æ–¹å°-å®‰å¾½é¢‘é“
                ah_lines.append(process_name_string(line.strip()))
            elif channel_name in hain_dictionary and check_url_existence(hain_lines, channel_address):  #åœ°æ–¹å°-æµ·å—é¢‘é“
                hain_lines.append(process_name_string(line.strip()))
            elif channel_name in nm_dictionary and check_url_existence(nm_lines, channel_address):  #åœ°æ–¹å°-å†…è’™é¢‘é“
                nm_lines.append(process_name_string(line.strip()))
            elif channel_name in ln_dictionary and check_url_existence(ln_lines, channel_address):  #åœ°æ–¹å°-è¾½å®é¢‘é“
                ln_lines.append(process_name_string(line.strip()))
            elif channel_name in sx_dictionary and check_url_existence(sx_lines, channel_address):  #åœ°æ–¹å°-é™•è¥¿é¢‘é“
                sx_lines.append(process_name_string(line.strip()))
            elif channel_name in shanxi_dictionary and check_url_existence(shanxi_lines, channel_address):  #åœ°æ–¹å°-å±±è¥¿é¢‘é“
                shanxi_lines.append(process_name_string(line.strip()))
            elif channel_name in shandong_dictionary and check_url_existence(shandong_lines, channel_address):  #åœ°æ–¹å°-å±±ä¸œé¢‘é“
                shandong_lines.append(process_name_string(line.strip()))
            elif channel_name in yunnan_dictionary and check_url_existence(yunnan_lines, channel_address):  #åœ°æ–¹å°-äº‘å—é¢‘é“
                yunnan_lines.append(process_name_string(line.strip()))
            elif channel_name in bj_dictionary and check_url_existence(bj_lines, channel_address):  #åœ°æ–¹å°-åŒ—äº¬é¢‘é“
                bj_lines.append(process_name_string(line.strip()))
            elif channel_name in cq_dictionary and check_url_existence(cq_lines, channel_address):  #åœ°æ–¹å°-é‡åº†é¢‘é“
                cq_lines.append(process_name_string(line.strip()))
            elif channel_name in fj_dictionary and check_url_existence(fj_lines, channel_address):  #åœ°æ–¹å°-ç¦å»ºé¢‘é“
                fj_lines.append(process_name_string(line.strip()))
            elif channel_name in gs_dictionary and check_url_existence(gs_lines, channel_address):  #åœ°æ–¹å°-ç”˜è‚ƒé¢‘é“
                gs_lines.append(process_name_string(line.strip()))
            elif channel_name in gx_dictionary and check_url_existence(gx_lines, channel_address):  #åœ°æ–¹å°-å¹¿è¥¿é¢‘é“
                gx_lines.append(process_name_string(line.strip()))
            elif channel_name in gz_dictionary and check_url_existence(gz_lines, channel_address):  #åœ°æ–¹å°-è´µå·é¢‘é“
                gz_lines.append(process_name_string(line.strip()))
            elif channel_name in heb_dictionary and check_url_existence(heb_lines, channel_address):  #åœ°æ–¹å°-æ²³åŒ—é¢‘é“
                heb_lines.append(process_name_string(line.strip()))
            elif channel_name in hen_dictionary and check_url_existence(hen_lines, channel_address):  #åœ°æ–¹å°-æ²³å—é¢‘é“
                hen_lines.append(process_name_string(line.strip()))
            elif channel_name in hlj_dictionary and check_url_existence(hlj_lines, channel_address):  #åœ°æ–¹å°-é»‘é¾™æ±Ÿé¢‘é“
                hlj_lines.append(process_name_string(line.strip()))
            elif channel_name in jl_dictionary and check_url_existence(jl_lines, channel_address):  #åœ°æ–¹å°-å‰æ—é¢‘é“
                jl_lines.append(process_name_string(line.strip()))
            elif channel_name in nx_dictionary and check_url_existence(nx_lines, channel_address):  #åœ°æ–¹å°-å®å¤é¢‘é“
                nx_lines.append(process_name_string(line.strip()))
            elif channel_name in jx_dictionary and check_url_existence(jx_lines, channel_address):  #åœ°æ–¹å°-æ±Ÿè¥¿é¢‘é“
                jx_lines.append(process_name_string(line.strip()))
            elif channel_name in qh_dictionary and check_url_existence(qh_lines, channel_address):  #åœ°æ–¹å°-é’æµ·é¢‘é“
                qh_lines.append(process_name_string(line.strip()))
            elif channel_name in sc_dictionary and check_url_existence(sc_lines, channel_address):  #åœ°æ–¹å°-å››å·é¢‘é“
                sc_lines.append(process_name_string(line.strip()))
            elif channel_name in sh_dictionary and check_url_existence(sh_lines, channel_address):  #åœ°æ–¹å°-ä¸Šæµ·é¢‘é“
                sh_lines.append(process_name_string(line.strip()))
            elif channel_name in tj_dictionary and check_url_existence(tj_lines, channel_address):  #åœ°æ–¹å°-å¤©æ´¥é¢‘é“
                tj_lines.append(process_name_string(line.strip()))
            elif channel_name in xj_dictionary and check_url_existence(xj_lines, channel_address):  #åœ°æ–¹å°-æ–°ç–†é¢‘é“ ADDã€2025-07-21 13:14:15ã€‘
                xj_lines.append(process_name_string(line.strip()))
            elif channel_name in sz_dictionary and check_url_existence(sz_lines, channel_address):  #æ•°å­—é¢‘é“
                sz_lines.append(process_name_string(line.strip()))
            elif channel_name in gj_dictionary and check_url_existence(gj_lines, channel_address):  #å›½é™…é¢‘é“
                gj_lines.append(process_name_string(line.strip()))
            elif channel_name in ty_dictionary and check_url_existence(ty_lines, channel_address):  #ä½“è‚²é¢‘é“
                ty_lines.append(process_name_string(line.strip()))
            elif any(tyss_dictionary in channel_name for tyss_dictionary in tyss_dictionary) and check_url_existence(tyss_lines, channel_address):  #ä½“è‚²èµ›äº‹
                tyss_lines.append(process_name_string(line.strip()))
            elif channel_name in dy_dictionary and check_url_existence(dy_lines, channel_address):  #ç”µå½±
                dy_lines.append(process_name_string(line.strip()))
            elif channel_name in dsj_dictionary and check_url_existence(dsj_lines, channel_address):  #ç”µè§†å‰§
                dsj_lines.append(process_name_string(line.strip()))
            elif channel_name in gat_dictionary and check_url_existence(gat_lines, channel_address):  #æ¸¯æ¾³å°
                gat_lines.append(process_name_string(line.strip()))
            elif channel_name in xg_dictionary and check_url_existence(xg_lines, channel_address):  #é¦™æ¸¯
                xg_lines.append(process_name_string(line.strip()))
            elif channel_name in aomen_dictionary and check_url_existence(aomen_lines, channel_address):  #æ¾³é—¨
                aomen_lines.append(process_name_string(line.strip()))
            elif channel_name in tw_dictionary and check_url_existence(tw_lines, channel_address):  #å°æ¹¾
                tw_lines.append(process_name_string(line.strip()))
            elif channel_name in jlp_dictionary and check_url_existence(jlp_lines, channel_address):  #çºªå½•ç‰‡
                jlp_lines.append(process_name_string(line.strip()))
            elif channel_name in dhp_dictionary and check_url_existence(dhp_lines, channel_address):  #åŠ¨ç”»ç‰‡
                dhp_lines.append(process_name_string(line.strip()))
            elif channel_name in xq_dictionary and check_url_existence(xq_lines, channel_address):  #æˆæ›²
                xq_lines.append(process_name_string(line.strip()))
            elif channel_name in js_dictionary and check_url_existence(js_lines, channel_address):  #è§£è¯´
                js_lines.append(process_name_string(line.strip()))
            elif channel_name in cw_dictionary and check_url_existence(cw_lines, channel_address):  #æ˜¥æ™š
                cw_lines.append(process_name_string(line.strip()))
            elif channel_name in douyu_dictionary and check_url_existence(douyu_lines, channel_address):  #æ–—é±¼ç›´æ’­
                douyu_lines.append(process_name_string(line.strip()))
            elif channel_name in huya_dictionary and check_url_existence(huya_lines, channel_address):  #è™ç‰™ç›´æ’­
                huya_lines.append(process_name_string(line.strip()))
            elif channel_name in zy_dictionary and check_url_existence(zy_lines, channel_address):  #ç»¼è‰ºé¢‘é“
                zy_lines.append(process_name_string(line.strip()))
            elif channel_name in yy_dictionary and check_url_existence(yy_lines, channel_address):  #éŸ³ä¹é¢‘é“
                yy_lines.append(process_name_string(line.strip()))
            elif channel_name in game_dictionary and check_url_existence(game_lines, channel_address):  #æ¸¸æˆé¢‘é“
                game_lines.append(process_name_string(line.strip()))
            elif channel_name in radio_dictionary and check_url_existence(radio_lines, channel_address):  #æ”¶éŸ³æœº
                radio_lines.append(process_name_string(line.strip()))
            elif channel_name in zb_dictionary and check_url_existence(zb_lines, channel_address):  #ç›´æ’­ä¸­å›½
                zb_lines.append(process_name_string(line.strip()))
            else:
                if channel_address not in other_lines_url:
                    other_lines_url.append(channel_address)   #è®°å½•å·²åŠ url
                    other_lines.append(line.strip())

# =====================
# ç½‘ç»œè¯·æ±‚å¤„ç†
# =====================

def get_random_user_agent():
    """éšæœºè·å–User-Agent"""
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36",
    ]
    return random.choice(USER_AGENTS)

def process_url(url):
    """å¤„ç†å•ä¸ªURLï¼Œè·å–å¹¶è§£æç›´æ’­æº"""
    try:
        other_lines.append("â—†â—†â—†ã€€" + url)  # è®°å½•å¤„ç†çš„URL
        
        req = urllib.request.Request(url)
        req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')

        with urllib.request.urlopen(req) as response:
            data = response.read()
            text = data.decode('utf-8').strip()
            
            # å¤„ç†M3Uæ ¼å¼
            is_m3u = text.startswith("#EXTM3U") or text.startswith("#EXTINF")
            if get_url_file_extension(url) in [".m3u", ".m3u8"] or is_m3u:
                text = convert_m3u_to_txt(text)
            
            # é€è¡Œå¤„ç†å†…å®¹
            lines = text.split('\n')
            print(f"å¤„ç†è¡Œæ•°: {len(lines)}")
            
            for line in lines:
                if "#genre#" not in line and "," in line and "://" in line and "tvbus://" not in line and "/udp/" not in line:
                    channel_name, channel_address = line.split(',', 1)
                    
                    # å¤„ç†åŠ é€Ÿæºï¼ˆåŒ…å«#å·çš„URLï¼‰
                    if "#" not in channel_address:
                        process_channel_line(line)
                    else:
                        url_list = channel_address.split('#')
                        for channel_url in url_list:
                            newline = f'{channel_name},{channel_url}'
                            process_channel_line(newline)
            
            other_lines.append('\n')  # URLå¤„ç†å®Œæˆåˆ†éš”ç¬¦

    except Exception as e:
        print(f"å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

def get_http_response(url, timeout=8, retries=2, backoff_factor=1.0):
    """å¸¦é‡è¯•æœºåˆ¶çš„HTTPè¯·æ±‚"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
    }
    
    for attempt in range(retries):
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=timeout) as response:
                data = response.read()
                return data.decode('utf-8')
        except urllib.error.HTTPError as e:
            print(f"[HTTPError] ä»£ç : {e.code}, URL: {url}")
            break
        except urllib.error.URLError as e:
            print(f"[URLError] åŸå› : {e.reason}, å°è¯•: {attempt + 1}")
        except socket.timeout:
            print(f"[Timeout] URL: {url}, å°è¯•: {attempt + 1}")
        except Exception as e:
            print(f"[Exception] {type(e).__name__}: {e}, å°è¯•: {attempt + 1}")
        
        # é‡è¯•ç­‰å¾…
        if attempt < retries - 1:
            time.sleep(backoff_factor * (2 ** attempt))
    
    return None

# =====================
# æ•°æ®æ’åºå’Œçº é”™
# =====================

def load_corrections_name(filename):
    """åŠ è½½é¢‘é“åç§°çº é”™å­—å…¸"""
    corrections = {}
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():
                continue
            parts = line.strip().split(',')
            correct_name = parts[0]
            for name in parts[1:]:
                corrections[name] = correct_name
    return corrections

def correct_name_data(corrections, data):
    """çº æ­£é¢‘é“åç§°æ•°æ®"""
    corrected_data = []
    for line in data:
        line = line.strip()
        if ',' not in line:
            continue

        name, url = line.split(',', 1)
        
        # åº”ç”¨åç§°çº æ­£
        if name in corrections and name != corrections[name]:
            name = corrections[name]

        corrected_data.append(f"{name},{url}")
    return corrected_data

def sort_data(order, data):
    """æ ¹æ®æŒ‡å®šé¡ºåºå¯¹æ•°æ®è¿›è¡Œæ’åº"""
    order_dict = {name: i for i, name in enumerate(order)}
    
    def sort_key(line):
        name = line.split(',')[0]
        return order_dict.get(name, len(order))
    
    sorted_data = sorted(data, key=sort_key)
    return sorted_data

# =====================
# æ—¥æœŸæ ¼å¼åŒ–
# =====================

def normalize_date_to_md(text):
    """å°†æ—¥æœŸç»Ÿä¸€æ ¼å¼åŒ–ä¸ºMM-DDæ ¼å¼"""
    text = text.strip()

    def format_md(m):
        month = int(m.group(1))
        day = int(m.group(2))
        after = m.group(3) or ''
        if not after.startswith(' '):
            after = ' ' + after
        return f"{month}-{day}{after}"

    # å¤„ç†å„ç§æ—¥æœŸæ ¼å¼
    text = re.sub(r'^0?(\d{1,2})/0?(\d{1,2})(.*)', format_md, text)  # MM/DD
    text = re.sub(r'^\d{4}-0?(\d{1,2})-0?(\d{1,2})(.*)', format_md, text)  # YYYY-MM-DD
    text = re.sub(r'^0?(\d{1,2})æœˆ0?(\d{1,2})æ—¥(.*)', format_md, text)  # ä¸­æ–‡æ—¥æœŸ

    return text

# =====================
# ç½‘é¡µç”ŸæˆåŠŸèƒ½
# =====================

def generate_playlist_html(data_list, output_file='playlist.html'):
    """ç”Ÿæˆä½“è‚²èµ›äº‹ç½‘é¡µ"""
    html_head = '''
    <!DOCTYPE html>
    <html lang="zh">
    <head>
        <meta charset="UTF-8">        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6061710286208572"
     crossorigin="anonymous"></script>
        <!-- Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-BS1Z4F5BDN"></script>
        <script> 
        window.dataLayer = window.dataLayer || []; 
        function gtag(){dataLayer.push(arguments);} 
        gtag('js', new Date()); 
        gtag('config', 'G-BS1Z4F5BDN'); 
        </script>
        <title>æœ€æ–°ä½“è‚²èµ›äº‹</title>
        <style>
            body { font-family: sans-serif; padding: 20px; background: #f9f9f9; }
            .item { margin-bottom: 20px; padding: 12px; background: #fff; border-radius: 8px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.06); }
            .title { font-weight: bold; font-size: 1.1em; color: #333; margin-bottom: 5px; }
            .url-wrapper { display: flex; align-items: center; gap: 10px; }
            .url {
                max-width: 80%;
                white-space: nowrap;
                overflow: hidden;
                text-overflow: ellipsis;
                font-size: 0.9em;
                color: #555;
                background: #f0f0f0;
                padding: 6px;
                border-radius: 4px;
                flex-grow: 1;
            }
            .copy-btn {
                background-color: #007BFF;
                border: none;
                color: white;
                padding: 6px 10px;
                border-radius: 4px;
                cursor: pointer;
                font-size: 0.8em;
            }
            .copy-btn:hover {
                background-color: #0056b3;
            }
        </style>
    </head>
    <body>
    <h2>ğŸ“‹ æœ€æ–°ä½“è‚²èµ›äº‹åˆ—è¡¨</h2>
    '''

    html_body = ''
    for idx, entry in enumerate(data_list):
        if ',' not in entry:
            continue
        info, url = entry.split(',', 1)
        url_id = f"url_{idx}"
        html_body += f'''
        <div class="item">
            <div class="title">ğŸ•’ {info}</div>
            <div class="url-wrapper">
                <div class="url" id="{url_id}">{url}</div>
                <button class="copy-btn" onclick="copyToClipboard('{url_id}')">å¤åˆ¶</button>
            </div>
        </div>
        '''

    html_tail = '''
    <script>
        function copyToClipboard(id) {
            const el = document.getElementById(id);
            const text = el.textContent;
            navigator.clipboard.writeText(text).then(() => {
                alert("å·²å¤åˆ¶é“¾æ¥ï¼");
            }).catch(err => {
                alert("å¤åˆ¶å¤±è´¥: " + err);
            });
        }
    </script>
    </body>
    </html>
    '''

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_head + html_body + html_tail)
    print(f"âœ… ç½‘é¡µå·²ç”Ÿæˆï¼š{output_file}")

# ====================
# å·¥å…·å‡½æ•°
# ====================

def get_random_url(file_path):
    """ä»æ–‡ä»¶ä¸­éšæœºè·å–ä¸€ä¸ªURL"""
    urls = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            url = line.strip().split(',')[-1]
            urls.append(url)    
    return random.choice(urls) if urls else None

def get_logo_by_channel_name(channel_name):
    """æ ¹æ®é¢‘é“åç§°è·å–logo URL"""
    for line in channels_logos:
        if not line.strip():
            continue
        name, url = line.split(',')
        if name == channel_name:
            return url
    return None

def make_m3u(txt_file, m3u_file):
    """å°†TXTæ–‡ä»¶è½¬æ¢ä¸ºM3Uæ ¼å¼"""
    try:
        output_text = '#EXTM3U x-tvg-url="https://live.fanmingming.cn/e.xml"\n'

        with open(txt_file, "r", encoding='utf-8') as file:
            input_text = file.read()

        lines = input_text.strip().split("\n")
        group_name = ""
        for line in lines:
            parts = line.split(",")
            if len(parts) == 2 and "#genre#" in line:
                group_name = parts[0]
            elif len(parts) == 2:
                channel_name = parts[0]
                channel_url = parts[1]
                logo_url = get_logo_by_channel_name(channel_name)
                if logo_url is None:
                    output_text += f'#EXTINF:-1 group-title="{group_name}",{channel_name}\n'
                    output_text += f'{channel_url}\n'
                else:
                    output_text += f'#EXTINF:-1 tvg-name="{channel_name}" tvg-logo="{logo_url}" group-title="{group_name}",{channel_name}\n'
                    output_text += f'{channel_url}\n'

        with open(f"{m3u_file}", "w", encoding='utf-8') as file:
            file.write(output_text)

        print(f"M3Uæ–‡ä»¶ '{m3u_file}' ç”ŸæˆæˆåŠŸã€‚")
    except Exception as e:
        print(f"ç”ŸæˆM3Uæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")

# ====================
# ä¸»ç¨‹åºæ‰§è¡Œ
# ====================

if __name__ == "__main__":
    # è·å–å½“å‰å·¥ä½œç›®å½•
    current_directory = os.getcwd()

    # åŠ è½½é¢‘é“å­—å…¸æ–‡ä»¶
    ys_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/CCTV.txt') #ä»…æ’åºç”¨
    ws_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/å«è§†é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    zj_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/æµ™æ±Ÿé¢‘é“.txt') #è¿‡æ»¤+æ’åº
    jsu_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/æ±Ÿè‹é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    gd_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/å¹¿ä¸œé¢‘é“.txt') #è¿‡æ»¤+æ’åº
    gx_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/å¹¿è¥¿é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    jx_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/æ±Ÿè¥¿é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    hb_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/æ¹–åŒ—é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    hn_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/æ¹–å—é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    ah_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/å®‰å¾½é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    hain_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/æµ·å—é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    nm_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/å†…è’™é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    ln_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/è¾½å®é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    sx_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/é™•è¥¿é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    shandong_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/å±±ä¸œé¢‘é“.txt') #è¿‡æ»¤+æ’åº
    shanxi_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/å±±è¥¿é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    hen_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/æ²³å—é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    heb_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/æ²³åŒ—é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    yunnan_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/äº‘å—é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    gz_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/è´µå·é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    sc_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/å››å·é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    fj_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/ç¦å»ºé¢‘é“.txt') #è¿‡æ»¤+æ’åº
    gs_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/ç”˜è‚ƒé¢‘é“.txt') #è¿‡æ»¤+æ’åº
    hlj_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/é»‘é¾™æ±Ÿé¢‘é“.txt') #è¿‡æ»¤+æ’åº
    jl_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/å‰æ—é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    nx_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/å®å¤é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    qh_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/é’æµ·é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    xj_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/æ–°ç–†é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    bj_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/åŒ—äº¬é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    sh_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/ä¸Šæµ·é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    tj_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/å¤©æ´¥é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    cq_dictionary=read_txt_to_array('scripts/livesource3/åœ°æ–¹å°/é‡åº†é¢‘é“.txt') #è¿‡æ»¤+æ’åº

    cw_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/æ˜¥æ™š.txt') #è¿‡æ»¤+æ’åº
    dy_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/ç”µå½±.txt') #è¿‡æ»¤+æ’åº
    dsj_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/ç”µè§†å‰§.txt') #è¿‡æ»¤+æ’åº
    gat_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/æ¸¯æ¾³å°.txt') #è¿‡æ»¤+æ’åº
    xg_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/é¦™æ¸¯.txt') #è¿‡æ»¤+æ’åº
    aomen_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/æ¾³é—¨.txt') #è¿‡æ»¤+æ’åº
    tw_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/å°æ¹¾.txt') #è¿‡æ»¤+æ’åº
    dhp_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/åŠ¨ç”»ç‰‡.txt') #è¿‡æ»¤+æ’åº
    radio_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/æ”¶éŸ³æœº.txt') #è¿‡æ»¤+æ’åº
    sz_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/æ•°å­—é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    gj_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/å›½é™…é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    ty_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/ä½“è‚²é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    tyss_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/ä½“è‚²èµ›äº‹.txt') #è¿‡æ»¤+æ’åº
    yy_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/éŸ³ä¹é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    js_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/è§£è¯´é¢‘é“.txt') #è¿‡æ»¤+æ’åº
    douyu_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/æ–—é±¼ç›´æ’­.txt') #è¿‡æ»¤+æ’åº
    huya_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/è™ç‰™ç›´æ’­.txt') #è¿‡æ»¤+æ’åº
    zb_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/ç›´æ’­ä¸­å›½.txt') #è¿‡æ»¤+æ’åº
    jlp_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/çºªå½•ç‰‡.txt') #è¿‡æ»¤+æ’åº
    zy_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/ç»¼è‰ºé¢‘é“.txt') #è¿‡æ»¤+æ’åº
    game_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/æ¸¸æˆé¢‘é“.txt') #è¿‡æ»¤+æ’åº
    xq_dictionary=read_txt_to_array('scripts/livesource3/ä¸»é¢‘é“/æˆæ›²é¢‘é“.txt') #è¿‡æ»¤+æ’åº

    # åŠ è½½é¢‘é“åç§°çº é”™å­—å…¸
    corrections_name = load_corrections_name('scripts/livesource3/corrections_name.txt')

    # åŠ è½½é¢‘é“logoåˆ—è¡¨ï¼Œæ–‡ä»¶ä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»ºç©ºæ–‡ä»¶
    logo_file_path = 'output/livesource3/channels_logos.txt'  # æ”¹ä¸ºè¾“å‡ºç›®å½•
    if os.path.exists(logo_file_path):
        channels_logos = read_txt_to_array(logo_file_path)
    else:
        os.makedirs(os.path.dirname(logo_file_path), exist_ok=True)  # ç¡®ä¿outputç›®å½•å·²åˆ›å»º
        with open(logo_file_path, 'w', encoding='utf-8') as f:
            f.write('')
        channels_logos = []

    # å¤„ç†è¿œç¨‹URLæº
    urls = read_txt_to_array('scripts/livesource3/urls-daily.txt')
    for url in urls:
        if url.startswith("http"):
            # å¤„ç†æ—¥æœŸå˜é‡æ›¿æ¢
            if "{MMdd}" in url:
                current_date_str = datetime.now().strftime("%m%d")
                url = url.replace("{MMdd}", current_date_str)
            if "{MMdd-1}" in url:
                yesterday_date_str = (datetime.now() - timedelta(days=1)).strftime("%m%d")
                url = url.replace("{MMdd-1}", yesterday_date_str)
            
            print(f"å¤„ç†URL: {url}")
            process_url(url)

    # å¤„ç†AKTVæº
    aktv_lines = []
    aktv_url = "https://aktv.space/live.m3u"
    aktv_text = get_http_response(aktv_url)
    if aktv_text:
        print("AKTVæˆåŠŸè·å–å†…å®¹")
        aktv_text = convert_m3u_to_txt(aktv_text)
        aktv_lines = aktv_text.strip().split('\n')
    else:
        print("AKTVè¯·æ±‚å¤±è´¥ï¼Œä»æœ¬åœ°è·å–ï¼")
        aktv_lines = read_txt_to_array('scripts/livesource3/æ‰‹å·¥åŒº/AKTV.txt')

    # å¤„ç†ç™½åå•é«˜å“åº”æº
    print(f"æ·»åŠ ç™½åå•é«˜å“åº”æº")
    whitelist_auto_lines = read_txt_to_array('scripts/livesource3/blacklist/whitelist_auto.txt')
    for whitelist_line in whitelist_auto_lines:
        if "#genre#" not in whitelist_line and "," in whitelist_line and "://" in whitelist_line:
            whitelist_parts = whitelist_line.split(",")
            try:
                response_time = float(whitelist_parts[0].replace("ms", ""))
            except ValueError:
                print(f"å“åº”æ—¶é—´è½¬æ¢å¤±è´¥: {whitelist_line}")
                response_time = 60000  # é»˜è®¤60ç§’
            if response_time < 2000:  # 2ç§’ä»¥å†…çš„é«˜å“åº”æº
                process_channel_line(",".join(whitelist_parts[1:]))

    # å¤„ç†æ‰‹å·¥åŒºé¢‘é“
    print(f"å¤„ç†æ‰‹å·¥åŒº...")
    zj_lines.extend(read_txt_to_array('scripts/livesource3/æ‰‹å·¥åŒº/æµ™æ±Ÿé¢‘é“.txt'))
    hb_lines.extend(read_txt_to_array('scripts/livesource3/æ‰‹å·¥åŒº/æ¹–åŒ—é¢‘é“.txt'))
    gd_lines.extend(read_txt_to_array('scripts/livesource3/æ‰‹å·¥åŒº/å¹¿ä¸œé¢‘é“.txt'))
    sh_lines.extend(read_txt_to_array('scripts/livesource3/æ‰‹å·¥åŒº/ä¸Šæµ·é¢‘é“.txt'))
    jsu_lines.extend(read_txt_to_array('scripts/livesource3/æ‰‹å·¥åŒº/æ±Ÿè‹é¢‘é“.txt'))

    # æ—¥æœŸæ ¼å¼åŒ–å¤„ç†
    normalized_tyss_lines = [normalize_date_to_md(s) for s in tyss_lines]

    # ç”Ÿæˆä½“è‚²èµ›äº‹ç½‘é¡µ
    generate_playlist_html(sorted(set(normalized_tyss_lines)), 'output/livesource3/sports.html')

    # å‡†å¤‡ç‰ˆæœ¬ä¿¡æ¯å’Œæ¨èå†…å®¹
    utc_time = datetime.now(timezone.utc)
    beijing_time = utc_time + timedelta(hours=8)
    formatted_time = beijing_time.strftime("%Y%m%d %H:%M:%S")

    version = formatted_time + "," + get_random_url('scripts/livesource3/ä»Šæ—¥æ¨å°.txt')
    about = "xiaoranmuze," + get_random_url('scripts/livesource3/ä»Šæ—¥æ¨å°.txt')

    daily_mtv = "ä»Šæ—¥æ¨è," + get_random_url('scripts/livesource3/ä»Šæ—¥æ¨è.txt')
    daily_mtv1 = "ğŸ”¥ä½è°ƒ," + get_random_url('scripts/livesource3/ä»Šæ—¥æ¨è.txt')
    daily_mtv2 = "ğŸ”¥ä½¿ç”¨," + get_random_url('scripts/livesource3/ä»Šæ—¥æ¨è.txt')
    daily_mtv3 = "ğŸ”¥ç¦æ­¢," + get_random_url('scripts/livesource3/ä»Šæ—¥æ¨è.txt')
    daily_mtv4 = "ğŸ”¥è´©å–," + get_random_url('scripts/livesource3/ä»Šæ—¥æ¨è.txt')

    # ====================
    # æ„å»ºè¾“å‡ºå†…å®¹
    # ====================

    # å…¨é›†ç‰ˆè¾“å‡º
all_lines = ["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(ys_dictionary, correct_name_data(corrections_name, ys_lines)) + ['\n'] + \
    ["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(ws_dictionary, correct_name_data(corrections_name, ws_lines)) + ['\n'] + \
    ["â˜˜ï¸æ¹–åŒ—é¢‘é“,#genre#"] + sort_data(hb_dictionary, set(correct_name_data(corrections_name, hb_lines))) + ['\n'] + \
    ["â˜˜ï¸æ¹–å—é¢‘é“,#genre#"] + sort_data(hn_dictionary, set(correct_name_data(corrections_name, hn_lines))) + ['\n'] + \
    ["â˜˜ï¸æµ™æ±Ÿé¢‘é“,#genre#"] + sort_data(zj_dictionary, set(correct_name_data(corrections_name, zj_lines))) + ['\n'] + \
    ["â˜˜ï¸å¹¿ä¸œé¢‘é“,#genre#"] + sort_data(gd_dictionary, set(correct_name_data(corrections_name, gd_lines))) + ['\n'] + \
    ["â˜˜ï¸æ±Ÿè‹é¢‘é“,#genre#"] + sort_data(jsu_dictionary, set(correct_name_data(corrections_name, jsu_lines))) + ['\n'] + \
    ["â˜˜ï¸æ±Ÿè¥¿é¢‘é“,#genre#"] + sort_data(jx_dictionary, set(correct_name_data(corrections_name, jx_lines))) + ['\n'] + \
    ["â˜˜ï¸åŒ—äº¬é¢‘é“,#genre#"] + sort_data(bj_dictionary, set(correct_name_data(corrections_name, bj_lines))) + ['\n'] + \
    ["â˜˜ï¸ä¸Šæµ·é¢‘é“,#genre#"] + sort_data(sh_dictionary, set(correct_name_data(corrections_name, sh_lines))) + ['\n'] + \
    ["â˜˜ï¸å¤©æ´¥é¢‘é“,#genre#"] + sort_data(tj_dictionary, set(correct_name_data(corrections_name, tj_lines))) + ['\n'] + \
    ["â˜˜ï¸é‡åº†é¢‘é“,#genre#"] + sort_data(cq_dictionary, set(correct_name_data(corrections_name, cq_lines))) + ['\n'] + \
    ["â˜˜ï¸å®‰å¾½é¢‘é“,#genre#"] + sort_data(ah_dictionary, set(correct_name_data(corrections_name, ah_lines))) + ['\n'] + \
    ["â˜˜ï¸æµ·å—é¢‘é“,#genre#"] + sort_data(hain_dictionary, set(correct_name_data(corrections_name, hain_lines))) + ['\n'] + \
    ["â˜˜ï¸å†…è’™é¢‘é“,#genre#"] + sort_data(nm_dictionary, set(correct_name_data(corrections_name, nm_lines))) + ['\n'] + \
    ["â˜˜ï¸è¾½å®é¢‘é“,#genre#"] + sort_data(ln_dictionary, set(correct_name_data(corrections_name, ln_lines))) + ['\n'] + \
    ["â˜˜ï¸é™•è¥¿é¢‘é“,#genre#"] + sort_data(sx_dictionary, set(correct_name_data(corrections_name, sx_lines))) + ['\n'] + \
    ["â˜˜ï¸å±±ä¸œé¢‘é“,#genre#"] + sort_data(shandong_dictionary, set(correct_name_data(corrections_name, shandong_lines))) + ['\n'] + \
    ["â˜˜ï¸å±±è¥¿é¢‘é“,#genre#"] + sort_data(shanxi_dictionary, set(correct_name_data(corrections_name, shanxi_lines))) + ['\n'] + \
    ["â˜˜ï¸äº‘å—é¢‘é“,#genre#"] + sort_data(yunnan_dictionary, set(correct_name_data(corrections_name, yunnan_lines))) + ['\n'] + \
    ["â˜˜ï¸ç¦å»ºé¢‘é“,#genre#"] + sort_data(fj_dictionary, set(correct_name_data(corrections_name, fj_lines))) + ['\n'] + \
    ["â˜˜ï¸ç”˜è‚ƒé¢‘é“,#genre#"] + sort_data(gs_dictionary, set(correct_name_data(corrections_name, gs_lines))) + ['\n'] + \
    ["â˜˜ï¸å¹¿è¥¿é¢‘é“,#genre#"] + sort_data(gx_dictionary, set(correct_name_data(corrections_name, gx_lines))) + ['\n'] + \
    ["â˜˜ï¸è´µå·é¢‘é“,#genre#"] + sort_data(gz_dictionary, set(correct_name_data(corrections_name, gz_lines))) + ['\n'] + \
    ["â˜˜ï¸æ²³åŒ—é¢‘é“,#genre#"] + sort_data(heb_dictionary, set(correct_name_data(corrections_name, heb_lines))) + ['\n'] + \
    ["â˜˜ï¸æ²³å—é¢‘é“,#genre#"] + sort_data(hen_dictionary, set(correct_name_data(corrections_name, hen_lines))) + ['\n'] + \
    ["â˜˜ï¸å‰æ—é¢‘é“,#genre#"] + sort_data(jl_dictionary, set(correct_name_data(corrections_name, jl_lines))) + ['\n'] + \
    ["â˜˜ï¸å®å¤é¢‘é“,#genre#"] + sort_data(nx_dictionary, set(correct_name_data(corrections_name, nx_lines))) + ['\n'] + \
    ["â˜˜ï¸é’æµ·é¢‘é“,#genre#"] + sort_data(qh_dictionary, set(correct_name_data(corrections_name, qh_lines))) + ['\n'] + \
    ["â˜˜ï¸å››å·é¢‘é“,#genre#"] + sort_data(sc_dictionary, set(correct_name_data(corrections_name, sc_lines))) + ['\n'] + \
    ["â˜˜ï¸æ–°ç–†é¢‘é“,#genre#"] + sort_data(xj_dictionary, set(correct_name_data(corrections_name, xj_lines))) + ['\n'] + \
    ["â˜˜ï¸é»‘é¾™æ±Ÿå°,#genre#"] + sorted(set(correct_name_data(corrections_name, hlj_lines))) + ['\n'] + \
    ["ğŸï¸æ•°å­—é¢‘é“,#genre#"] + sort_data(sz_dictionary, set(correct_name_data(corrections_name, sz_lines))) + ['\n'] + \
    ["ğŸŒå›½é™…é¢‘é“,#genre#"] + sort_data(gj_dictionary, set(correct_name_data(corrections_name, gj_lines))) + ['\n'] + \
    ["âš½ä½“è‚²é¢‘é“,#genre#"] + sort_data(ty_dictionary, set(correct_name_data(corrections_name, ty_lines))) + ['\n'] + \
    ["ğŸ†ä½“è‚²èµ›äº‹,#genre#"] + normalized_tyss_lines + ['\n'] + \
    ["ğŸ¬æ–—é±¼ç›´æ’­,#genre#"] + sort_data(douyu_dictionary, set(correct_name_data(corrections_name, douyu_lines))) + ['\n'] + \
    ["ğŸ¯è™ç‰™ç›´æ’­,#genre#"] + sort_data(huya_dictionary, set(correct_name_data(corrections_name, huya_lines))) + ['\n'] + \
    ["ğŸ™ï¸è§£è¯´é¢‘é“,#genre#"] + sort_data(js_dictionary, set(correct_name_data(corrections_name, js_lines))) + ['\n'] + \
    ["ğŸ¬ç”µå½±é¢‘é“,#genre#"] + sort_data(dy_dictionary, set(correct_name_data(corrections_name, dy_lines))) + ['\n'] + \
    ["ğŸ“ºç”µÂ·è§†Â·å‰§,#genre#"] + sort_data(dsj_dictionary, set(correct_name_data(corrections_name, dsj_lines))) + ['\n'] + \
    ["ğŸ“½ï¸è®°Â·å½•Â·ç‰‡,#genre#"] + sort_data(jlp_dictionary,set(correct_name_data(corrections_name,jlp_lines)))+ ['\n'] + \
    ["ğŸ•åŠ¨Â·ç”»Â·ç‰‡,#genre#"] + sort_data(dhp_dictionary, set(correct_name_data(corrections_name, dhp_lines))) + ['\n'] + \
    ["ğŸ“»æ”¶Â·éŸ³Â·æœº,#genre#"] + sort_data(radio_dictionary, set(correct_name_data(corrections_name, radio_lines))) + ['\n'] + \
    ["ğŸ‡¨ğŸ‡³æ¸¯Â·æ¾³Â·å°,#genre#"] +read_txt_to_array('æ‰‹å·¥åŒº/â™ªæ¸¯æ¾³å°.txt') + sort_data(gat_dictionary, set(correct_name_data(corrections_name, gat_lines))) + aktv_lines + ['\n'] + \
    ["ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“,#genre#"] + sort_data(xg_dictionary, set(correct_name_data(corrections_name, xg_lines))) + ['\n'] + \
    ["ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“,#genre#"] + sort_data(aomen_dictionary, set(correct_name_data(corrections_name, aomen_lines))) + aktv_lines + ['\n'] + \
    ["ğŸ‡¹ğŸ‡¼å°æ¹¾é¢‘é“,#genre#"] + sort_data(tw_dictionary, set(correct_name_data(corrections_name, tw_lines)))  + ['\n'] + \
    ["ğŸ­æˆæ›²é¢‘é“,#genre#"] + sort_data(xq_dictionary,set(correct_name_data(corrections_name,xq_lines))) + ['\n'] + \
    ["ğŸµéŸ³ä¹é¢‘é“,#genre#"] + sort_data(yy_dictionary, set(correct_name_data(corrections_name, yy_lines))) + ['\n'] + \
    ["ğŸ¤ç»¼è‰ºé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name,zy_lines))) + ['\n'] + \
    ["ğŸ®æ¸¸æˆé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name,game_lines))) + ['\n'] + \
    ["âœ¨ä¼˜è´¨å¤®è§†,#genre#"] + read_txt_to_array('æ‰‹å·¥åŒº/â™ªä¼˜è´¨å¤®è§†.txt') + ['\n'] + \
    ["ğŸ›°ï¸ä¼˜è´¨å«è§†,#genre#"] + read_txt_to_array('æ‰‹å·¥åŒº/â™ªä¼˜è´¨å«è§†.txt') + ['\n'] + \
    ["ğŸ“¹ç›´æ’­ä¸­å›½,#genre#"] + sort_data(zb_dictionary, set(correct_name_data(corrections_name, zb_lines))) + ['\n'] + \
    ["ğŸ§¨å†å±Šæ˜¥æ™š,#genre#"] + sort_data(cw_dictionary, set(correct_name_data(corrections_name, cw_lines))) + ['\n'] + \
    ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [daily_mtv] + [daily_mtv1] + [daily_mtv2] + [daily_mtv3] + [daily_mtv4] + read_txt_to_array('æ‰‹å·¥åŒº/about.txt') + ['\n']

    # ç²¾ç®€ç‰ˆè¾“å‡º
all_lines_simple = ["å¤®è§†é¢‘é“,#genre#"] + sort_data(ys_dictionary, correct_name_data(corrections_name, ys_lines)) + ['\n'] + \
    ["å«è§†é¢‘é“,#genre#"] + sort_data(ws_dictionary, correct_name_data(corrections_name, ws_lines)) + ['\n'] + \
    ["åœ°æ–¹é¢‘é“,#genre#"] + \
    sort_data(hb_dictionary, set(correct_name_data(corrections_name, hb_lines))) + \
    sort_data(hn_dictionary, set(correct_name_data(corrections_name, hn_lines))) + \
    sort_data(zj_dictionary, set(correct_name_data(corrections_name, zj_lines))) + \
    sort_data(gd_dictionary, set(correct_name_data(corrections_name, gd_lines))) + \
    sort_data(shandong_dictionary, set(correct_name_data(corrections_name, shandong_lines))) + \
    sorted(set(correct_name_data(corrections_name, jsu_lines))) + \
    sorted(set(correct_name_data(corrections_name, ah_lines))) + \
    sorted(set(correct_name_data(corrections_name, hain_lines))) + \
    sorted(set(correct_name_data(corrections_name, nm_lines))) + \
    sorted(set(correct_name_data(corrections_name, ln_lines))) + \
    sorted(set(correct_name_data(corrections_name, sx_lines))) + \
    sorted(set(correct_name_data(corrections_name, shanxi_lines))) + \
    sorted(set(correct_name_data(corrections_name, yunnan_lines))) + \
    sorted(set(correct_name_data(corrections_name, bj_lines))) + \
    sorted(set(correct_name_data(corrections_name, cq_lines))) + \
    sorted(set(correct_name_data(corrections_name, fj_lines))) + \
    sorted(set(correct_name_data(corrections_name, gs_lines))) + \
    sorted(set(correct_name_data(corrections_name, gx_lines))) + \
    sorted(set(correct_name_data(corrections_name, gz_lines))) + \
    sorted(set(correct_name_data(corrections_name, heb_lines))) + \
    sorted(set(correct_name_data(corrections_name, hen_lines))) + \
    sorted(set(correct_name_data(corrections_name, jl_lines))) + \
    sorted(set(correct_name_data(corrections_name, jx_lines))) + \
    sorted(set(correct_name_data(corrections_name, nx_lines))) + \
    sorted(set(correct_name_data(corrections_name, qh_lines))) + \
    sorted(set(correct_name_data(corrections_name, sc_lines))) + \
    sorted(set(correct_name_data(corrections_name, tj_lines))) + \
    sorted(set(correct_name_data(corrections_name, xj_lines))) + \
    sorted(set(correct_name_data(corrections_name, hlj_lines))) + \
    ['\n'] + \
    ["æ•°å­—é¢‘é“,#genre#"] + sort_data(sz_dictionary, set(correct_name_data(corrections_name, sz_lines))) + ['\n'] + \
    ["æ›´æ–°æ—¶é—´,#genre#"] + [version] + ['\n']

    # å®šåˆ¶ç‰ˆè¾“å‡º
all_lines_custom = ["ğŸŒå¤®è§†é¢‘é“,#genre#"] + sort_data(ys_dictionary, correct_name_data(corrections_name, ys_lines)) + ['\n'] + \
    ["ğŸ“¡å«è§†é¢‘é“,#genre#"] + sort_data(ws_dictionary, correct_name_data(corrections_name, ws_lines)) + ['\n'] + \
    ["ğŸ åœ°æ–¹é¢‘é“,#genre#"] + \
    sort_data(hb_dictionary, set(correct_name_data(corrections_name, hb_lines))) + \
    sort_data(hn_dictionary, set(correct_name_data(corrections_name, hn_lines))) + \
    sort_data(zj_dictionary, set(correct_name_data(corrections_name, zj_lines))) + \
    sort_data(gd_dictionary, set(correct_name_data(corrections_name, gd_lines))) + \
    sort_data(shandong_dictionary, set(correct_name_data(corrections_name, shandong_lines))) + \
    sorted(set(correct_name_data(corrections_name, jsu_lines))) + \
    sorted(set(correct_name_data(corrections_name, ah_lines))) + \
    sorted(set(correct_name_data(corrections_name, hain_lines))) + \
    sorted(set(correct_name_data(corrections_name, nm_lines))) + \
    sorted(set(correct_name_data(corrections_name, ln_lines))) + \
    sorted(set(correct_name_data(corrections_name, sx_lines))) + \
    sorted(set(correct_name_data(corrections_name, shanxi_lines))) + \
    sorted(set(correct_name_data(corrections_name, yunnan_lines))) + \
    sorted(set(correct_name_data(corrections_name, bj_lines))) + \
    sorted(set(correct_name_data(corrections_name, cq_lines))) + \
    sorted(set(correct_name_data(corrections_name, fj_lines))) + \
    sorted(set(correct_name_data(corrections_name, gs_lines))) + \
    sorted(set(correct_name_data(corrections_name, gx_lines))) + \
    sorted(set(correct_name_data(corrections_name, gz_lines))) + \
    sorted(set(correct_name_data(corrections_name, heb_lines))) + \
    sorted(set(correct_name_data(corrections_name, hen_lines))) + \
    sorted(set(correct_name_data(corrections_name, jl_lines))) + \
    sorted(set(correct_name_data(corrections_name, jx_lines))) + \
    sorted(set(correct_name_data(corrections_name, nx_lines))) + \
    sorted(set(correct_name_data(corrections_name, qh_lines))) + \
    sorted(set(correct_name_data(corrections_name, sc_lines))) + \
    sorted(set(correct_name_data(corrections_name, tj_lines))) + \
    sorted(set(correct_name_data(corrections_name, xj_lines))) + \
    sorted(set(correct_name_data(corrections_name, hlj_lines))) + \
    ['\n'] + \
    ["ğŸï¸æ•°å­—é¢‘é“,#genre#"] + sort_data(sz_dictionary, set(correct_name_data(corrections_name, sz_lines))) + ['\n'] + \
    ["ğŸŒå›½é™…é¢‘é“,#genre#"] + sort_data(gj_dictionary, set(correct_name_data(corrections_name, gj_lines))) + ['\n'] + \
    ["âš½ä½“è‚²é¢‘é“,#genre#"] + sort_data(ty_dictionary, set(correct_name_data(corrections_name, ty_lines))) + ['\n'] + \
    ["ğŸ†ä½“è‚²èµ›äº‹,#genre#"] + normalized_tyss_lines + ['\n'] + \
    ["ğŸ¬æ–—é±¼ç›´æ’­,#genre#"] + sort_data(douyu_dictionary, set(correct_name_data(corrections_name, douyu_lines))) + ['\n'] + \
    ["ğŸ¯è™ç‰™ç›´æ’­,#genre#"] + sort_data(huya_dictionary, set(correct_name_data(corrections_name, huya_lines))) + ['\n'] + \
    ["ğŸ™ï¸è§£è¯´é¢‘é“,#genre#"] + sort_data(js_dictionary, set(correct_name_data(corrections_name, js_lines))) + ['\n'] + \
    ["ğŸ¬ç”µå½±é¢‘é“,#genre#"] + sort_data(dy_dictionary, set(correct_name_data(corrections_name, dy_lines))) + ['\n'] + \
    ["ğŸ“ºç”µÂ·è§†Â·å‰§,#genre#"] + sort_data(dsj_dictionary, set(correct_name_data(corrections_name, dsj_lines))) + ['\n'] + \
    ["ğŸ“½ï¸è®°Â·å½•Â·ç‰‡,#genre#"] + sort_data(jlp_dictionary,set(correct_name_data(corrections_name,jlp_lines)))+ ['\n'] + \
    ["ğŸ•åŠ¨Â·ç”»Â·ç‰‡,#genre#"] + sort_data(dhp_dictionary, set(correct_name_data(corrections_name, dhp_lines))) + ['\n'] + \
    ["ğŸ“»æ”¶Â·éŸ³Â·æœº,#genre#"] + sort_data(radio_dictionary, set(correct_name_data(corrections_name, radio_lines))) + ['\n'] + \
    ["ğŸ‡¨ğŸ‡³æ¸¯Â·æ¾³Â·å°,#genre#"] +read_txt_to_array('æ‰‹å·¥åŒº/â™ªæ¸¯æ¾³å°.txt') + sort_data(gat_dictionary, set(correct_name_data(corrections_name, gat_lines))) + aktv_lines + ['\n'] + \
    ["ğŸ‡­ğŸ‡°é¦™æ¸¯é¢‘é“,#genre#"] + sort_data(xg_dictionary, set(correct_name_data(corrections_name, xg_lines))) + ['\n'] + \
    ["ğŸ‡²ğŸ‡´æ¾³é—¨é¢‘é“,#genre#"] + sort_data(aomen_dictionary, set(correct_name_data(corrections_name, aomen_lines))) + aktv_lines + ['\n'] + \
    ["ğŸ‡¹ğŸ‡¼å°æ¹¾é¢‘é“,#genre#"] + sort_data(tw_dictionary, set(correct_name_data(corrections_name, tw_lines)))  + ['\n'] + \
    ["ğŸ­æˆæ›²é¢‘é“,#genre#"] + sort_data(xq_dictionary,set(correct_name_data(corrections_name,xq_lines))) + ['\n'] + \
    ["ğŸµéŸ³ä¹é¢‘é“,#genre#"] + sort_data(yy_dictionary, set(correct_name_data(corrections_name, yy_lines))) + ['\n'] + \
    ["ğŸ¤ç»¼è‰ºé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name,zy_lines))) + ['\n'] + \
    ["ğŸ®æ¸¸æˆé¢‘é“,#genre#"] + sorted(set(correct_name_data(corrections_name,game_lines))) + ['\n'] + \
    ["âœ¨ä¼˜è´¨å¤®è§†,#genre#"] + read_txt_to_array('æ‰‹å·¥åŒº/â™ªä¼˜è´¨å¤®è§†.txt') + ['\n'] + \
    ["ğŸ›°ï¸ä¼˜è´¨å«è§†,#genre#"] + read_txt_to_array('æ‰‹å·¥åŒº/â™ªä¼˜è´¨å«è§†.txt') + ['\n'] + \
    ["ğŸ“¹ç›´æ’­ä¸­å›½,#genre#"] + sort_data(zb_dictionary, set(correct_name_data(corrections_name, zb_lines))) + ['\n'] + \
    ["ğŸ§¨å†å±Šæ˜¥æ™š,#genre#"] + sort_data(cw_dictionary, set(correct_name_data(corrections_name, cw_lines))) + ['\n'] + \
    ["ğŸ•’æ›´æ–°æ—¶é—´,#genre#"] + [version] + [about] + [daily_mtv] + [daily_mtv1] + [daily_mtv2] + [daily_mtv3] + [daily_mtv4] + read_txt_to_array('æ‰‹å·¥åŒº/about.txt') + ['\n']

    # ====================
    # æ–‡ä»¶è¾“å‡º
    # ====================

    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
new_output_file = "output/livesource3/full.txt"
new_output_file_lite = "output/livesource3/lite.txt"
new_output_file_custom = "output/livesource3/custom.txt"
others_file = "output/livesource3/others.txt"

    try:
        # å†™å…¥ç²¾ç®€ç‰ˆ
        with open(new_output_file_lite, 'w', encoding='utf-8') as f:
            for line in all_lines_lite:
                f.write(line + '\n')
        print(f"ç²¾ç®€ç‰ˆå·²ä¿å­˜: {new_output_file_lite}")

        # å†™å…¥å…¨é›†ç‰ˆ
        with open(new_output_file, 'w', encoding='utf-8') as f:
            for line in all_lines:
                f.write(line + '\n')
        print(f"å…¨é›†ç‰ˆå·²ä¿å­˜: {new_output_file}")

        # å†™å…¥å…¶ä»–æº
        with open(others_file, 'w', encoding='utf-8') as f:
            for line in other_lines:
                f.write(line + '\n')
        print(f"å…¶ä»–æºå·²ä¿å­˜: {others_file}")

        # å†™å…¥å®šåˆ¶ç‰ˆ
        with open(new_output_file_custom, 'w', encoding='utf-8') as f:
            for line in all_lines_custom:
                f.write(line + '\n')
        print(f"å®šåˆ¶ç‰ˆå·²ä¿å­˜: {new_output_file_custom}")

        # ç”ŸæˆM3Uæ–‡ä»¶
        make_m3u(new_output_file, new_output_file.replace(".txt", ".m3u"))
        make_m3u(new_output_file_lite, new_output_file_lite.replace(".txt", ".m3u"))
        make_m3u(new_output_file_custom, new_output_file_custom.replace(".txt", ".m3u"))

    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

    # ====================
    # æ‰§è¡Œç»Ÿè®¡
    # ====================

    timeend = datetime.now()
    elapsed_time = timeend - timestart
    total_seconds = elapsed_time.total_seconds()
    minutes = int(total_seconds // 60)
    seconds = int(total_seconds % 60)
    
    timestart_str = timestart.strftime("%Y%m%d_%H_%M_%S")
    timeend_str = timeend.strftime("%Y%m%d_%H_%M_%S")

    print(f"å¼€å§‹æ—¶é—´: {timestart_str}")
    print(f"ç»“æŸæ—¶é—´: {timeend_str}")
    print(f"æ‰§è¡Œæ—¶é—´: {minutes} åˆ† {seconds} ç§’")

    # ç»Ÿè®¡ä¿¡æ¯
    combined_blacklist_hj = len(combined_blacklist)
    all_lines_hj = len(all_lines)
    other_lines_hj = len(other_lines)
    all_lines_custom_hj = len(all_lines_custom)
    
    print(f"é»‘åå•è¡Œæ•°: {combined_blacklist_hj}")
    print(f"å…¨é›†ç‰ˆè¡Œæ•°: {all_lines_hj}")
    print(f"å…¶ä»–æºè¡Œæ•°: {other_lines_hj}")
    print(f"å®šåˆ¶ç‰ˆè¡Œæ•°: {all_lines_custom_hj}")