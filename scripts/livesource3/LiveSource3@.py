import urllib.request
from urllib.parse import urlparse
import re
import os
from datetime import datetime, timedelta, timezone
import random
import opencc
import socket
import time
import logging
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import functools
from typing import List, Set, Dict, Any

# ==================== é…ç½®åŒºåŸŸ - åœ¨è¿™é‡Œä¿®æ”¹ ====================

# ã€ä¿®æ”¹ä½ç½®1ã€‘è¾“å‡ºç›®å½•é…ç½®
OUTPUT_DIR = '../../output/livesource3/'  # ä»scripts/livesource3/åˆ°output/livesource3/
ASSETS_DIR = './'  # å½“å‰ç›®å½•å°±æ˜¯scripts/livesource3/
BLACKLIST_DIR = 'blacklist/'  # é»‘åå•åœ¨scripts/livesource3/blacklist/

# ã€ä¿®æ”¹ä½ç½®2ã€‘é¢‘é“æ–‡ä»¶è·¯å¾„é…ç½® - åœ¨è¿™é‡Œæ·»åŠ æˆ–åˆ é™¤é¢‘é“åˆ†ç±»
CHANNEL_DIRS = {
    'ys': 'ä¸»é¢‘é“/CCTV.txt',           # å¤®è§†é¢‘é“
    'ws': 'ä¸»é¢‘é“/å«è§†é¢‘é“.txt',        # å«è§†é¢‘é“  
    'hb': 'åœ°æ–¹å°/æ¹–åŒ—é¢‘é“.txt',        # æ¹–åŒ—é¢‘é“
    'hn': 'åœ°æ–¹å°/æ¹–å—é¢‘é“.txt',        # æ¹–å—é¢‘é“
    'tyss': 'ä¸»é¢‘é“/ä½“è‚²èµ›äº‹.txt',      # ä½“è‚²èµ›äº‹
    # åœ¨è¿™é‡Œæ·»åŠ æ–°çš„é¢‘é“åˆ†ç±»ï¼Œæ ¼å¼ï¼š'ä»£å·': 'æ–‡ä»¶è·¯å¾„'
    # ä¾‹å¦‚ï¼š'zj': 'åœ°æ–¹å°/æµ™æ±Ÿé¢‘é“.txt'
}

# ã€ä¿®æ”¹ä½ç½®3ã€‘æ‰‹å·¥æ•°æ®æ–‡ä»¶é…ç½® - åœ¨è¿™é‡Œæ·»åŠ æ‰‹å·¥ç»´æŠ¤çš„æ–‡ä»¶
MANUAL_DIRS = {
    'hubei': 'æ‰‹å·¥åŒº/æ‰‹å·¥é¢‘é“.txt',     # æ‰‹å·¥é¢‘é“æ•°æ®
    'aktv': 'æ‰‹å·¥åŒº/AKTV.txt',         # AKTVæ•°æ®
    'about': 'æ‰‹å·¥åŒº/about.txt',       # å…³äºä¿¡æ¯
    # åœ¨è¿™é‡Œæ·»åŠ æ–°çš„æ‰‹å·¥æ•°æ®æ–‡ä»¶
}

# ã€ä¿®æ”¹ä½ç½®4ã€‘ç½‘ç»œè¯·æ±‚é…ç½®
TIMEOUT = 8
MAX_RETRIES = 2
MAX_WORKERS = 5
WHITELIST_THRESHOLD = 2000  # ç™½åå•å“åº”æ—¶é—´é˜ˆå€¼(ms)

# ã€ä¿®æ”¹ä½ç½®5ã€‘éœ€è¦æ¸…ç†çš„å­—ç¬¦åˆ—è¡¨ - åœ¨è¿™é‡Œæ·»åŠ è¦æ¸…ç†çš„å­—ç¬¦
REMOVAL_LIST = [
    "_ç”µä¿¡", "ç”µä¿¡", "é«˜æ¸…", "é¢‘é“", "ï¼ˆHDï¼‰", "-HD", "è‹±é™†", "_ITV", "(åŒ—ç¾)", "(HK)", 
    "AKtv", "ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "é¢‘é™†", "å¤‡é™†", "å£¹é™†", "è´°é™†", "åé™†", "è‚†é™†", 
    "ä¼é™†", "é™†é™†", "æŸ’é™†", "é¢‘æ™´", "é¢‘ç²¤", "[è¶…æ¸…]", "é«˜æ¸…", "è¶…æ¸…", "æ ‡æ¸…", "æ–¯ç‰¹",
    "ç²¤é™†", "å›½é™†", "è‚†æŸ’", "é¢‘è‹±", "é¢‘ç‰¹", "é¢‘å›½", "é¢‘å£¹", "é¢‘è´°", "è‚†è´°", "é¢‘æµ‹", 
    "å’ªå’•", "é—½ç‰¹", "é«˜ç‰¹", "é¢‘é«˜", "é¢‘æ ‡", "æ±é˜³"
    # åœ¨è¿™é‡Œæ·»åŠ æ–°çš„éœ€è¦æ¸…ç†çš„å­—ç¬¦
]

# ã€ä¿®æ”¹ä½ç½®6ã€‘è¾“å…¥URLæ–‡ä»¶ - åœ¨è¿™é‡Œä¿®æ”¹URLæ¥æºæ–‡ä»¶
URLS_FILE = 'urls-daily.txt'

# ã€ä¿®æ”¹ä½ç½®7ã€‘å…¶ä»–èµ„æºæ–‡ä»¶
CORRECTIONS_FILE = 'corrections_name.txt'
TODAY_RECOMMEND_FILE = 'ä»Šæ—¥æ¨è.txt'
TODAY_PUSH_FILE = 'ä»Šæ—¥æ¨å°.txt'
LOGO_FILE = 'logo.txt'

# ==================== é…ç½®ç±» ====================
class Config:
    """é…ç½®ç®¡ç†ç±»"""
    OUTPUT_DIR = OUTPUT_DIR
    ASSETS_DIR = ASSETS_DIR
    BLACKLIST_DIR = BLACKLIST_DIR
    CHANNEL_DIRS = CHANNEL_DIRS
    MANUAL_DIRS = MANUAL_DIRS
    TIMEOUT = TIMEOUT
    MAX_RETRIES = MAX_RETRIES
    MAX_WORKERS = MAX_WORKERS
    WHITELIST_THRESHOLD = WHITELIST_THRESHOLD
    REMOVAL_LIST = REMOVAL_LIST
    URLS_FILE = URLS_FILE
    CORRECTIONS_FILE = CORRECTIONS_FILE
    TODAY_RECOMMEND_FILE = TODAY_RECOMMEND_FILE
    TODAY_PUSH_FILE = TODAY_PUSH_FILE
    LOGO_FILE = LOGO_FILE

# ==================== æ—¥å¿—è®¾ç½® ====================
def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'{Config.OUTPUT_DIR}processing.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

# ==================== å·¥å…·ç±» ====================
class URLTracker:
    """URLè·Ÿè¸ªå™¨ï¼Œç”¨äºå»é‡"""
    def __init__(self):
        self.seen_urls: Set[str] = set()
    
    def add_url(self, url: str) -> bool:
        """æ·»åŠ URLå¹¶è¿”å›æ˜¯å¦ä¸ºæ–°URL"""
        if url in self.seen_urls:
            return False
        self.seen_urls.add(url)
        return True

class ProcessingStats:
    """å¤„ç†ç»Ÿè®¡ç±»"""
    def __init__(self):
        self.start_time = datetime.now()
        self.processed_urls = 0
        self.successful_urls = 0
        self.total_lines = 0
        self.categories = defaultdict(int)
    
    def log_final_stats(self):
        """è®°å½•æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        elapsed = datetime.now() - self.start_time
        total_seconds = elapsed.total_seconds()
        minutes = int(total_seconds // 60)
        seconds = int(total_seconds % 60)
        
        logging.info(f"""
å¤„ç†ç»Ÿè®¡:
- æ€»URLæ•°: {self.processed_urls}
- æˆåŠŸURLæ•°: {self.successful_urls}
- æ€»è¡Œæ•°: {self.total_lines}
- è€—æ—¶: {minutes}åˆ†{seconds}ç§’
- æˆåŠŸç‡: {(self.successful_urls/max(1, self.processed_urls))*100:.1f}%
- åˆ†ç±»ç»Ÿè®¡: {dict(self.categories)}
        """)

# ==================== ç¼“å­˜è£…é¥°å™¨ ====================
@functools.lru_cache(maxsize=1000)
def traditional_to_simplified_cached(text: str) -> str:
    """ç¼“å­˜çš„ç¹ç®€è½¬æ¢"""
    converter = opencc.OpenCC('t2s')
    return converter.convert(text)

@functools.lru_cache(maxsize=500)
def get_logo_by_channel_name_cached(channel_name: str) -> str:
    """ç¼“å­˜çš„é¢‘é“logoæŸ¥è¯¢"""
    return get_logo_by_channel_name(channel_name)

# ==================== æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ====================
def read_txt_to_array(file_name: str) -> List[str]:
    """è¯»å–æ–‡æœ¬æ–‡ä»¶åˆ°æ•°ç»„ï¼Œæ”¯æŒç©ºè¡Œè¿‡æ»¤"""
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            return [line.strip() for line in file if line.strip()]
    except FileNotFoundError:
        logging.warning(f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_name}")
        return []
    except Exception as e:
        logging.error(f"è¯»å–æ–‡ä»¶é”™è¯¯ {file_name}: {e}")
        return []

def read_blacklist_from_txt(file_path: str) -> List[str]:
    """è¯»å–é»‘åå•æ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return [line.split(',')[1].strip() for line in file if ',' in line]
    except Exception as e:
        logging.error(f"è¯»å–é»‘åå•é”™è¯¯ {file_path}: {e}")
        return []

def robust_http_request(url: str, timeout: int = Config.TIMEOUT, retries: int = Config.MAX_RETRIES) -> str:
    """å¥å£®çš„ç½‘ç»œè¯·æ±‚å‡½æ•°"""
    headers = {'User-Agent': get_random_user_agent()}
    
    for attempt in range(retries + 1):
        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=timeout) as response:
                return response.read().decode('utf-8')
        except Exception as e:
            logging.warning(f"è¯·æ±‚å°è¯• {attempt + 1} å¤±è´¥ {url}: {e}")
            if attempt < retries:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
    
    logging.error(f"æ‰€æœ‰è¯·æ±‚å°è¯•éƒ½å¤±è´¥: {url}")
    return ""

def get_random_user_agent() -> str:
    """éšæœºUser-Agent"""
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36",
    ]
    return random.choice(USER_AGENTS)

def clean_url(url: str) -> str:
    """æ¸…ç†URLï¼Œç§»é™¤$ä¹‹åçš„å†…å®¹"""
    last_dollar_index = url.rfind('$')
    return url[:last_dollar_index] if last_dollar_index != -1 else url

def clean_channel_name(channel_name: str) -> str:
    """æ¸…ç†é¢‘é“åç§°"""
    for item in Config.REMOVAL_LIST:
        channel_name = channel_name.replace(item, "")
    
    # ç§»é™¤æœ«å°¾ç‰¹å®šå­—ç¬¦
    if channel_name.endswith("HD"):
        channel_name = channel_name[:-2]
    if channel_name.endswith("å°") and len(channel_name) > 3:
        channel_name = channel_name[:-1]
    
    return channel_name

def get_url_file_extension(url: str) -> str:
    """è·å–URLæ–‡ä»¶æ‰©å±•å"""
    path = urlparse(url).path
    return os.path.splitext(path)[1]

def convert_m3u_to_txt(m3u_content: str) -> str:
    """è½¬æ¢M3Uæ ¼å¼åˆ°TXTæ ¼å¼"""
    lines = m3u_content.split('\n')
    txt_lines = []
    channel_name = ""
    
    for line in lines:
        if line.startswith("#EXTM3U"):
            continue
        elif line.startswith("#EXTINF"):
            channel_name = line.split(',')[-1].strip()
        elif line.startswith(("http", "rtmp", "p3p")):
            txt_lines.append(f"{channel_name},{line.strip()}")
        elif "#genre#" not in line and "," in line and "://" in line:
            pattern = r'^[^,]+,[^\s]+://[^\s]+$'
            if re.match(pattern, line):
                txt_lines.append(line)
    
    return '\n'.join(txt_lines)

# ==================== é¢‘é“å¤„ç†ç±» ====================
class ChannelProcessor:
    """é¢‘é“å¤„ç†å™¨"""
    
    def __init__(self):
        self.stats = ProcessingStats()
        self.url_tracker = URLTracker()
        
        # åˆå§‹åŒ–æ•°æ®å­˜å‚¨ - ã€ä¿®æ”¹ä½ç½®8ã€‘åœ¨è¿™é‡Œæ·»åŠ æ–°çš„é¢‘é“åˆ†ç±»å­˜å‚¨
        self.ys_lines = []
        self.ws_lines = []
        self.hb_lines = []
        self.hn_lines = []
        self.ty_lines = []
        self.tyss_lines = []
        self.other_lines = []
        # åœ¨è¿™é‡Œæ·»åŠ æ–°çš„é¢‘é“åˆ†ç±»åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š
        # self.zj_lines = []  # æµ™æ±Ÿé¢‘é“
        
        # è¯»å–å­—å…¸å’Œé…ç½®
        self._load_dictionaries()
        self._load_blacklists()
        self._load_corrections()
        
    def _load_dictionaries(self):
        """åŠ è½½é¢‘é“å­—å…¸"""
        # ã€ä¿®æ”¹ä½ç½®9ã€‘åœ¨è¿™é‡Œæ·»åŠ æ–°çš„é¢‘é“å­—å…¸
        self.ys_dictionary = read_txt_to_array(Config.CHANNEL_DIRS['ys'])
        self.ws_dictionary = read_txt_to_array(Config.CHANNEL_DIRS['ws'])
        self.hb_dictionary = read_txt_to_array(Config.CHANNEL_DIRS['hb'])
        self.hn_dictionary = read_txt_to_array(Config.CHANNEL_DIRS['hn'])
        self.tyss_dictionary = read_txt_to_array(Config.CHANNEL_DIRS['tyss'])
        # åœ¨è¿™é‡Œæ·»åŠ æ–°çš„é¢‘é“å­—å…¸ï¼Œä¾‹å¦‚ï¼š
        # self.zj_dictionary = read_txt_to_array(Config.CHANNEL_DIRS['zj'])
    
    def _load_blacklists(self):
        """åŠ è½½é»‘åå•"""
        blacklist_auto = read_blacklist_from_txt(f'{Config.BLACKLIST_DIR}blacklist_auto.txt')
        blacklist_manual = read_blacklist_from_txt(f'{Config.BLACKLIST_DIR}blacklist_manual.txt')
        self.combined_blacklist = set(blacklist_auto + blacklist_manual)
        
        # åŠ è½½ç™½åå•
        self.whitelist_auto_lines = read_txt_to_array(f'{Config.BLACKLIST_DIR}whitelist_auto.txt')
    
    def _load_corrections(self):
        """åŠ è½½çº é”™é…ç½®"""
        self.corrections_name = self._load_corrections_name(Config.CORRECTIONS_FILE)
    
    def _load_corrections_name(self, filename: str) -> Dict[str, str]:
        """åŠ è½½åç§°çº é”™é…ç½®"""
        corrections = {}
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                for line in f:
                    if not line.strip():
                        continue
                    parts = line.strip().split(',')
                    correct_name = parts[0]
                    for name in parts[1:]:
                        corrections[name] = correct_name
        except Exception as e:
            logging.error(f"åŠ è½½çº é”™é…ç½®é”™è¯¯: {e}")
        return corrections

    def process_name_string(self, input_str: str) -> str:
        """å¤„ç†é¢‘é“åç§°å­—ç¬¦ä¸²"""
        parts = input_str.split(',')
        processed_parts = [self._process_part(part) for part in parts]
        return ','.join(processed_parts)
    
    def _process_part(self, part_str: str) -> str:
        """å¤„ç†å•ä¸ªé¢‘é“åç§°éƒ¨åˆ†"""
        if "CCTV" in part_str and "://" not in part_str:
            part_str = part_str.replace("IPV6", "").replace("PLUS", "+").replace("1080", "")
            filtered_str = ''.join(char for char in part_str if char.isdigit() or char in 'K+')
            
            if not filtered_str.strip():
                filtered_str = part_str.replace("CCTV", "")
            
            if len(filtered_str) > 2 and re.search(r'4K|8K', filtered_str):
                filtered_str = re.sub(r'(4K|8K).*', r'\1', filtered_str)
                if len(filtered_str) > 2:
                    filtered_str = re.sub(r'(4K|8K)', r'(\1)', filtered_str)
            
            return "CCTV" + filtered_str
        elif "å«è§†" in part_str:
            return re.sub(r'å«è§†ã€Œ.*ã€', 'å«è§†', part_str)
        
        return part_str

    def process_channel_line(self, line: str):
        """å¤„ç†å•è¡Œé¢‘é“æ•°æ®"""
        if not self._is_valid_line(line):
            return
        
        channel_name, channel_address = line.split(',', 1)
        channel_name = clean_channel_name(channel_name)
        channel_name = traditional_to_simplified_cached(channel_name)
        channel_address = clean_url(channel_address.strip())
        
        line = f"{channel_name},{channel_address}"
        
        if channel_address in self.combined_blacklist:
            return
        
        # åˆ†ç±»å¤„ç†
        category = self._categorize_channel(channel_name, channel_address)
        if category:
            processed_line = self.process_name_string(line)
            getattr(self, f'{category}_lines').append(processed_line)
            self.stats.categories[category] += 1
    
    def _is_valid_line(self, line: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆè¡Œ"""
        return (line and ',' in line and "://" in line and 
                "#genre#" not in line and "#EXTINF:" not in line and
                "tvbus://" not in line and "/udp/" not in line)
    
    def _categorize_channel(self, channel_name: str, channel_address: str) -> str:
        """é¢‘é“åˆ†ç±»"""
        # ã€ä¿®æ”¹ä½ç½®10ã€‘åœ¨è¿™é‡Œæ·»åŠ æ–°çš„é¢‘é“åˆ†ç±»é€»è¾‘
        if "CCTV" in channel_name and self.url_tracker.add_url(channel_address):
            return "ys"
        elif channel_name in self.ws_dictionary and self.url_tracker.add_url(channel_address):
            return "ws"
        elif channel_name in self.hn_dictionary and self.url_tracker.add_url(channel_address):
            return "hn"
        elif channel_name in self.hb_dictionary and self.url_tracker.add_url(channel_address):
            return "hb"
        elif any(tyss in channel_name for tyss in self.tyss_dictionary) and self.url_tracker.add_url(channel_address):
            return "tyss"
        # åœ¨è¿™é‡Œæ·»åŠ æ–°çš„åˆ†ç±»é€»è¾‘ï¼Œä¾‹å¦‚ï¼š
        # elif channel_name in self.zj_dictionary and self.url_tracker.add_url(channel_address):
        #     return "zj"
        elif self.url_tracker.add_url(channel_address):
            self.other_lines.append(f"{channel_name},{channel_address}")
            return "other"
        return ""

    def process_single_url(self, url: str):
        """å¤„ç†å•ä¸ªURL"""
        self.stats.processed_urls += 1
        logging.info(f"å¤„ç†URL: {url}")
        
        try:
            # å¤„ç†æ—¥æœŸæ¨¡æ¿
            url = self._process_date_templates(url)
            
            content = robust_http_request(url)
            if content:
                self._process_url_content(content)
                self.stats.successful_urls += 1
        except Exception as e:
            logging.error(f"å¤„ç†URLå¤±è´¥ {url}: {e}")
    
    def _process_date_templates(self, url: str) -> str:
        """å¤„ç†URLä¸­çš„æ—¥æœŸæ¨¡æ¿"""
        if "{MMdd}" in url:
            current_date_str = datetime.now().strftime("%m%d")
            url = url.replace("{MMdd}", current_date_str)
        if "{MMdd-1}" in url:
            yesterday_date_str = (datetime.now() - timedelta(days=1)).strftime("%m%d")
            url = url.replace("{MMdd-1}", yesterday_date_str)
        return url
    
    def _process_url_content(self, content: str):
        """å¤„ç†URLå†…å®¹"""
        # å¤„ç†M3Uæ ¼å¼
        if content.startswith("#EXTM3U") or content.startswith("#EXTINF"):
            content = convert_m3u_to_txt(content)
        
        lines = content.split('\n')
        self.stats.total_lines += len(lines)
        
        for line in lines:
            if self._is_valid_line(line):
                if "#" not in line.split(',', 1)[1]:
                    self.process_channel_line(line)
                else:
                    self._process_hashed_urls(line)
    
    def _process_hashed_urls(self, line: str):
        """å¤„ç†å¸¦#å·çš„URL"""
        channel_name, channel_address = line.split(',', 1)
        url_list = channel_address.split('#')
        for channel_url in url_list:
            newline = f'{channel_name},{channel_url}'
            self.process_channel_line(newline)

    def process_whitelist(self):
        """å¤„ç†ç™½åå•"""
        logging.info("å¤„ç†ç™½åå•...")
        whitelist_count = 0
        
        for whitelist_line in self.whitelist_auto_lines:
            if not self._is_valid_line(whitelist_line):
                continue
            
            parts = whitelist_line.split(",")
            try:
                response_time = float(parts[0].replace("ms", ""))
                if response_time < Config.WHITELIST_THRESHOLD:
                    self.process_channel_line(",".join(parts[1:]))
                    whitelist_count += 1
            except ValueError:
                logging.warning(f"ç™½åå•å“åº”æ—¶é—´è½¬æ¢å¤±è´¥: {whitelist_line}")
        
        logging.info(f"ç™½åå•å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæº: {whitelist_count}")

    def process_manual_data(self):
        """å¤„ç†æ‰‹å·¥æ•°æ®"""
        logging.info("å¤„ç†æ‰‹å·¥æ•°æ®...")
        
        # ã€ä¿®æ”¹ä½ç½®11ã€‘åœ¨è¿™é‡Œæ·»åŠ æ–°çš„æ‰‹å·¥æ•°æ®å¤„ç†
        # æ‰‹å·¥é¢‘é“æ•°æ®
        manual_channels = read_txt_to_array(Config.MANUAL_DIRS['hubei'])
        for line in manual_channels:
            self.process_channel_line(line)
        
        # AKTVæ•°æ®
        self._process_aktv_data()
        
        # åœ¨è¿™é‡Œæ·»åŠ æ–°çš„æ‰‹å·¥æ•°æ®å¤„ç†
        # ä¾‹å¦‚ï¼šzj_manual = read_txt_to_array(Config.MANUAL_DIRS['zhejiang'])
    
    def _process_aktv_data(self):
        """å¤„ç†AKTVæ•°æ®"""
        aktv_url = "https://aktv.space/live.m3u"
        content = robust_http_request(aktv_url)
        
        if content:
            logging.info("AKTVåœ¨çº¿è·å–æˆåŠŸ")
            content = convert_m3u_to_txt(content)
            aktv_lines = content.strip().split('\n')
        else:
            logging.info("AKTVä½¿ç”¨æœ¬åœ°æ•°æ®")
            aktv_lines = read_txt_to_array(Config.MANUAL_DIRS['aktv'])
        
        for line in aktv_lines:
            self.process_channel_line(line)

# ==================== æ’åºå’Œè¾“å‡ºç±» ====================
class OutputGenerator:
    """è¾“å‡ºç”Ÿæˆå™¨"""
    
    def __init__(self, processor: ChannelProcessor):
        self.processor = processor
    
    def correct_name_data(self, data: List[str]) -> List[str]:
        """çº æ­£é¢‘é“åç§°"""
        corrected_data = []
        for line in data:
            if ',' not in line:
                continue
            name, url = line.split(',', 1)
            if name in self.processor.corrections_name:
                name = self.processor.corrections_name[name]
            corrected_data.append(f"{name},{url}")
        return corrected_data
    
    def sort_data(self, order: List[str], data: List[str]) -> List[str]:
        """æŒ‰æŒ‡å®šé¡ºåºæ’åºæ•°æ®"""
        order_dict = {name: i for i, name in enumerate(order)}
        
        def sort_key(line):
            name = line.split(',')[0]
            return order_dict.get(name, len(order))
        
        return sorted(data, key=sort_key)
    
    def _custom_sort(self, s: str) -> int:
        """è‡ªå®šä¹‰æ’åºå‡½æ•°"""
        if "CCTV-4K" in s:
            return 2
        elif "CCTV-8K" in s:
            return 3
        elif "(4K)" in s:
            return 1
        else:
            return 0
    
    def generate_output_files(self):
        """ç”Ÿæˆæ‰€æœ‰è¾“å‡ºæ–‡ä»¶"""
        logging.info("ç”Ÿæˆè¾“å‡ºæ–‡ä»¶...")
        
        # å¤„ç†æ—¥æœŸæ ¼å¼åŒ–
        normalized_tyss_lines = [self._normalize_date_to_md(s) for s in self.processor.tyss_lines]
        
        # ç”Ÿæˆç½‘é¡µ
        self._generate_sports_html(normalized_tyss_lines)
        
        # ç”Ÿæˆå„ç‰ˆæœ¬æ–‡ä»¶
        self._generate_version_files(normalized_tyss_lines)
        
        # ç”Ÿæˆothersæ–‡ä»¶
        self._generate_others_file()
    
    def _normalize_date_to_md(self, text: str) -> str:
        """æ—¥æœŸç»Ÿä¸€æ ¼å¼åŒ–ä¸ºMM-DDæ ¼å¼"""
        text = text.strip()
        
        def format_md(m):
            month = int(m.group(1))
            day = int(m.group(2))
            after = m.group(3) or ''
            if not after.startswith(' '):
                after = ' ' + after
            return f"{month}-{day}{after}"
        
        # å¤„ç†å„ç§æ—¥æœŸæ ¼å¼
        text = re.sub(r'^0?(\d{1,2})/0?(\d{1,2})(.*)', format_md, text)
        text = re.sub(r'^\d{4}-0?(\d{1,2})-0?(\d{1,2})(.*)', format_md, text)
        text = re.sub(r'^0?(\d{1,2})æœˆ0?(\d{1,2})æ—¥(.*)', format_md, text)
        
        return text
    
    def _generate_sports_html(self, tyss_lines: List[str]):
        """ç”Ÿæˆä½“è‚²èµ›äº‹ç½‘é¡µ"""
        generate_playlist_html(sorted(set(tyss_lines)), f'{Config.OUTPUT_DIR}sports.html')
    
    def _generate_version_files(self, normalized_tyss_lines: List[str]):
        """ç”Ÿæˆå„ç‰ˆæœ¬æ–‡ä»¶"""
        # è·å–åŠ¨æ€å†…å®¹
        version_info = self._get_version_info()
        about_info = self._get_about_info()
        daily_recommendations = self._get_daily_recommendations()
        
        # å…¨é›†ç‰ˆ
        full_content = self._build_full_content(normalized_tyss_lines, version_info, about_info, daily_recommendations)
        self._save_file('full.txt', full_content)
        
        # ç²¾ç®€ç‰ˆ  
        lite_content = self._build_lite_content(version_info)
        self._save_file('lite.txt', lite_content)
        
        # å®šåˆ¶ç‰ˆ
        custom_content = self._build_custom_content(normalized_tyss_lines, version_info, about_info, daily_recommendations)
        self._save_file('custom.txt', custom_content)
    
    def _get_version_info(self) -> str:
        """è·å–ç‰ˆæœ¬ä¿¡æ¯"""
        utc_time = datetime.now(timezone.utc)
        beijing_time = utc_time + timedelta(hours=8)
        formatted_time = beijing_time.strftime("%Y%m%d %H:%M:%S")
        random_url = self._get_random_url(Config.TODAY_PUSH_FILE)
        return f"{formatted_time},{random_url}"
    
    def _get_about_info(self) -> str:
        """è·å–å…³äºä¿¡æ¯"""
        random_url = self._get_random_url(Config.TODAY_PUSH_FILE)
        return f"xiaoranmuze,{random_url}"
    
    def _get_daily_recommendations(self) -> List[str]:
        """è·å–æ¯æ—¥æ¨è"""
        recommendations = []
        prefixes = ["ä»Šæ—¥æ¨è", "ğŸ”¥ä½è°ƒ", "ğŸ”¥ä½¿ç”¨", "ğŸ”¥ç¦æ­¢", "ğŸ”¥è´©å–"]
        
        for prefix in prefixes:
            random_url = self._get_random_url(Config.TODAY_RECOMMEND_FILE)
            if random_url:
                recommendations.append(f"{prefix},{random_url}")
        
        return recommendations
    
    def _get_random_url(self, file_path: str) -> str:
        """éšæœºè·å–URL"""
        urls = []
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                for line in file:
                    url = line.strip().split(',')[-1]
                    urls.append(url)
            return random.choice(urls) if urls else ""
        except Exception as e:
            logging.error(f"è·å–éšæœºURLå¤±è´¥ {file_path}: {e}")
            return ""
    
    def _build_full_content(self, tyss_lines: List[str], version: str, about: str, daily: List[str]) -> List[str]:
        """æ„å»ºå…¨é›†ç‰ˆå†…å®¹"""
        about_lines = read_txt_to_array(Config.MANUAL_DIRS['about'])
        
        content = [
            "ğŸŒå¤®è§†é¢‘é“,#genre#"
        ] + self.sort_data(self.processor.ys_dictionary, self.correct_name_data(self.processor.ys_lines)) + ['\n'] + [
            "ğŸ“¡å«è§†é¢‘é“,#genre#"
        ] + self.sort_data(self.processor.ws_dictionary, self.correct_name_data(self.processor.ws_lines)) + ['\n']
        
        # ã€ä¿®æ”¹ä½ç½®12ã€‘åœ¨è¿™é‡Œæ·»åŠ æ–°çš„é¢‘é“åˆ†ç±»åˆ°è¾“å‡º
        content += [
            "â˜˜ï¸æ¹–åŒ—é¢‘é“,#genre#"
        ] + self.sort_data(self.processor.hb_dictionary, set(self.correct_name_data(self.processor.hb_lines))) + ['\n'] + [
            "â˜˜ï¸æ¹–å—é¢‘é“,#genre#"
        ] + self.sort_data(self.processor.hn_dictionary, set(self.correct_name_data(self.processor.hn_lines))) + ['\n']
        
        # åœ¨è¿™é‡Œæ·»åŠ æ–°çš„é¢‘é“åˆ†ç±»ï¼Œä¾‹å¦‚ï¼š
        # content += [
        #     "ğŸæµ™æ±Ÿé¢‘é“,#genre#"
        # ] + self.sort_data(self.processor.zj_dictionary, set(self.correct_name_data(self.processor.zj_lines))) + ['\n']
        
        content += [
            "ğŸ†ä½“è‚²èµ›äº‹,#genre#"
        ] + tyss_lines + ['\n'] + [
            "ğŸ•’æ›´æ–°æ—¶é—´,#genre#"
        ] + [version, about] + daily + about_lines + ['\n']
        
        return content
    
    def _build_lite_content(self, version: str) -> List[str]:
        """æ„å»ºç²¾ç®€ç‰ˆå†…å®¹"""
        return [
            "å¤®è§†é¢‘é“,#genre#"
        ] + self.sort_data(self.processor.ys_dictionary, self.correct_name_data(self.processor.ys_lines)) + ['\n'] + [
            "å«è§†é¢‘é“,#genre#"
        ] + self.sort_data(self.processor.ws_dictionary, self.correct_name_data(self.processor.ws_lines)) + ['\n'] + [
            "æ›´æ–°æ—¶é—´,#genre#"
        ] + [version] + ['\n']
    
    def _build_custom_content(self, tyss_lines: List[str], version: str, about: str, daily: List[str]) -> List[str]:
        """æ„å»ºå®šåˆ¶ç‰ˆå†…å®¹"""
        about_lines = read_txt_to_array(Config.MANUAL_DIRS['about'])
        
        return [
            "ğŸŒå¤®è§†é¢‘é“,#genre#"
        ] + self.sort_data(self.processor.ys_dictionary, self.correct_name_data(self.processor.ys_lines)) + ['\n'] + [
            "ğŸ“¡å«è§†é¢‘é“,#genre#"
        ] + self.sort_data(self.processor.ws_dictionary, self.correct_name_data(self.processor.ws_lines)) + ['\n'] + [
            "ğŸ†ä½“è‚²èµ›äº‹,#genre#"
        ] + tyss_lines + ['\n'] + [
            "ğŸ•’æ›´æ–°æ—¶é—´,#genre#"
        ] + [version, about] + daily + about_lines + ['\n']
    
    def _save_file(self, filename: str, content: List[str]):
        """ä¿å­˜æ–‡ä»¶"""
        try:
            filepath = f"{Config.OUTPUT_DIR}{filename}"
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write('\n'.join(content))
            logging.info(f"æ–‡ä»¶å·²ä¿å­˜: {filepath}")
            
            # ç”Ÿæˆå¯¹åº”çš„M3Uæ–‡ä»¶
            m3u_file = filepath.replace(".txt", ".m3u")
            make_m3u(filepath, m3u_file)
        except Exception as e:
            logging.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {filename}: {e}")
    
    def _generate_others_file(self):
        """ç”Ÿæˆothersæ–‡ä»¶"""
        try:
            filepath = f"{Config.OUTPUT_DIR}others.txt"
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("å…¶å®ƒé¢‘é“,#genre#\n")
                for line in self.processor.other_lines:
                    if line and "," in line and "://" in line and not line.startswith("â—†â—†â—†"):
                        f.write(line + '\n')
            logging.info(f"Othersæ–‡ä»¶å·²ä¿å­˜: {filepath}")
            
            # ç”Ÿæˆå¯¹åº”çš„M3Uæ–‡ä»¶
            m3u_file = filepath.replace(".txt", ".m3u")
            make_m3u(filepath, m3u_file)
        except Exception as e:
            logging.error(f"ä¿å­˜Othersæ–‡ä»¶å¤±è´¥: {e}")

# ==================== ä¿ç•™çš„åŸæœ‰å‡½æ•° ====================
def generate_playlist_html(data_list, output_file='playlist.html'):
    """ç”Ÿæˆä½“è‚²èµ›äº‹ç½‘é¡µï¼ˆä¿ç•™åŸæœ‰å®ç°ï¼‰"""
    html_head = '''
    <!DOCTYPE html>
    <html lang="zh">
    <head>
        <meta charset="UTF-8">        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-6061710286208572"
     crossorigin="anonymous"></script>
        <!-- Setup Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-BS1Z4F5BDN"></script>
        <script> 
        window.dataLayer = window.dataLayer || []; 
        function gtag(){dataLayer.push(arguments);} 
        gtag('js', new Date()); 
        gtag('config', 'G-BS1Z4F5BDN'); 
        </script>
        <title>æœ€æ–°ä½“è‚²èµ›äº‹</title>
        <style>
            body { font-family: sans-serif; padding: 20px; background: #f9f9f9; }
            .item { margin-bottom: 20px; padding: 12px; background: #fff; border-radius: 8px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.06); }
            .title { font-weight: bold; font-size: 1.1em; color: #333; margin-bottom: 5px; }
            .url-wrapper { display: flex; align-items: center; gap: 10px; }
            .url {
                max-width: 80%;
                white-space: nowrap;
                overflow: hidden;
                text-overflow: ellipsis;
                font-size: 0.9em;
                color: #555;
                background: #f0f0f0;
                padding: 6px;
                border-radius: 4px;
                flex-grow: 1;
            }
            .copy-btn {
                background-color: #007BFF;
                border: none;
                color: white;
                padding: 6px 10px;
                border-radius: 4px;
                cursor: pointer;
                font-size: 0.8em;
            }
            .copy-btn:hover {
                background-color: #0056b3;
            }
        </style>
    </head>
    <body>
    <h2>ğŸ“‹ æœ€æ–°ä½“è‚²èµ›äº‹åˆ—è¡¨</h2>
    '''
    
    html_body = ''
    for idx, entry in enumerate(data_list):
        if ',' not in entry:
            continue
        info, url = entry.split(',', 1)
        url_id = f"url_{idx}"
        html_body += f'''
        <div class="item">
            <div class="title">ğŸ•’ {info}</div>
            <div class="url-wrapper">
                <div class="url" id="{url_id}">{url}</div>
                <button class="copy-btn" onclick="copyToClipboard('{url_id}')">å¤åˆ¶</button>
            </div>
        </div>
        '''
    
    html_tail = '''
    <script>
        function copyToClipboard(id) {
            const el = document.getElementById(id);
            const text = el.textContent;
            navigator.clipboard.writeText(text).then(() => {
                alert("å·²å¤åˆ¶é“¾æ¥ï¼");
            }).catch(err => {
                alert("å¤åˆ¶å¤±è´¥: " + err);
            });
        }
    </script>
    </body>
    </html>
    '''
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_head + html_body + html_tail)
    logging.info(f"ç½‘é¡µå·²ç”Ÿæˆ: {output_file}")

def get_logo_by_channel_name(channel_name):
    """è·å–é¢‘é“logoï¼ˆä¿ç•™åŸæœ‰å®ç°ï¼‰"""
    channels_logos = read_txt_to_array(Config.LOGO_FILE)
    for line in channels_logos:
        if not line.strip():
            continue
        name, url = line.split(',')
        if name == channel_name:
            return url
    return None

def make_m3u(txt_file, m3u_file):
    """ç”ŸæˆM3Uæ–‡ä»¶ï¼ˆä¿ç•™åŸæœ‰å®ç°ï¼‰"""
    try:
        output_text = '#EXTM3U x-tvg-url="https://live.fanmingming.cn/e.xml"\n'
        
        with open(txt_file, "r", encoding='utf-8') as file:
            input_text = file.read()
        
        lines = input_text.strip().split("\n")
        group_name = ""
        for line in lines:
            parts = line.split(",")
            if len(parts) == 2 and "#genre#" in line:
                group_name = parts[0]
            elif len(parts) == 2:
                channel_name = parts[0]
                channel_url = parts[1]
                logo_url = get_logo_by_channel_name_cached(channel_name)
                if logo_url is None:
                    output_text += f"#EXTINF:-1 group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"
                else:
                    output_text += f"#EXTINF:-1  tvg-name=\"{channel_name}\" tvg-logo=\"{logo_url}\"  group-title=\"{group_name}\",{channel_name}\n"
                    output_text += f"{channel_url}\n"
        
        with open(f"{m3u_file}", "w", encoding='utf-8') as file:
            file.write(output_text)
        
        logging.info(f"M3Uæ–‡ä»¶ç”ŸæˆæˆåŠŸ: {m3u_file}")
    except Exception as e:
        logging.error(f"ç”ŸæˆM3Uæ–‡ä»¶å¤±è´¥: {e}")

# ==================== ä¸»æ‰§è¡Œå‡½æ•° ====================
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    setup_logging()
    logging.info("å¼€å§‹å¤„ç†ç›´æ’­æº...")
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = ChannelProcessor()
    output_generator = OutputGenerator(processor)
    
    try:
        # 1. å¤„ç†URLåˆ—è¡¨
        urls = read_txt_to_array(Config.URLS_FILE)
        http_urls = [url for url in urls if url.startswith("http")]
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†URL
        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
            list(executor.map(processor.process_single_url, http_urls))
        
        # 2. å¤„ç†ç™½åå•
        processor.process_whitelist()
        
        # 3. å¤„ç†æ‰‹å·¥æ•°æ®
        processor.process_manual_data()
        
        # 4. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
        output_generator.generate_output_files()
        
        # 5. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        processor.stats.log_final_stats()
        
        logging.info("å¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        logging.error(f"ä¸»æ‰§è¡Œè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main()